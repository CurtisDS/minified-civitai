import re
import os
import json
import shlex
import base64
import zipfile
import tempfile
import argparse
import requests
import markdownify
from PIL import Image
from io import BytesIO
from datetime import datetime
from bs4 import BeautifulSoup

# Declare variables
review_limit = 35
comment_limit = 4
file_name_max_size = 60

def get_folder(type):
	if type.lower() == "textualinversion":
		folder = "embeddings"
	elif type.lower() == "hypernetwork":
		folder = "hypernetwork"
	elif type.lower() == "checkpoint":
		folder = "models"
	elif type.lower() == "lora":
		folder = "lora"
	else:
		folder = "unknownCivitai"
	return folder

class NewlineFormatter(argparse.RawDescriptionHelpFormatter):
	def _split_lines(self, text, width):
		return text.splitlines()

help_description = f'Get and parse Civitai model pages into HTML pages, markdown pages, and or save the source JSON from the website.\n\
This script takes the generated files for a single model and saves them in a folder depending on the type of model.\n\n\
Files will be saved in:\n\
  "./{get_folder("Checkpoint")}/" for Checkpoint\n\
  "./{get_folder("TextualInversion")}/" for TextualInversion\n\
  "./{get_folder("Hypernetwork")}/" for Hypernetwork\n\
  "./{get_folder("Lora")}/" for Lora\n\
\n\
If more than one model is specified than a zip file will be created instead containing the output with the same folder structure.'

help_epilog = f'This script can create a markdown (.md), HTML (.html), and JSON (.json) file for each model.\n\
You can control what files are output by setting flags for each file type you want generated by using command line arguments.\n\
If no output flags are set it will default to create every file type. You are able to use more than one output flag at once,\n\
but using all of them is equivalent to using none of them.'

parser = argparse.ArgumentParser(description=help_description, add_help=False, epilog=help_epilog, formatter_class=NewlineFormatter)
parser.add_argument('-j', '--json', action='store_true', help='turn on JSON output flag')
parser.add_argument('-h', '--html', action='store_true', help='turn on HTML output flag')
parser.add_argument('-m', '--md', action='store_true', help='turn on Markdown output flag')
parser.add_argument('-n', '--file_names', action='store_true', help='turn on "version files" list output flag (a file that will associate the civitai id to its model name hash and html file)')
parser.add_argument('-u', '--update', action='store_true', help='update models found in provided json files')
parser.add_argument('-f', '--force_new_arguments', action='store_true', help='on top of the initial arguments ask the user for new arguments once started (allows you to use a bat file to set default arguments but still add more when running)')
parser.add_argument('-p', '--print', action='store_true', help='turn on flag to print to console all model names and keys when processing multiple models at once')
parser.add_argument('-?', '--help', action='store_true', help='show this help message and exit')
parser.add_argument('args', nargs='*', help='model numbers or file paths to json files that contain previously downloaded data. If not specified you will be asked for this at runtime')
args = parser.parse_args()

# Load the image cache from a file if it exists
try:
	with open('cache.json', 'r') as f:
		image_cache = json.load(f)
except FileNotFoundError:
	image_cache = {}

try:
	with open('cache2.json', 'r') as f:
		image_cache2 = json.load(f)
except FileNotFoundError:
	image_cache2 = {}

image_cache_changed = False

# Load the file names previously found from a file if it exists
try:
	with open('version_files.json', 'r') as f:
		version_files = json.load(f)
except FileNotFoundError:
	version_files = {}

if args.help:
	# Show the help message and exit
	parser.print_help()
	exit()

if not any([args.json, args.html, args.md, args.file_names]):
	# Enable all output formats by default
	args.json = True
	args.html = True
	args.md = True
	args.file_names = True

if not args.args or args.force_new_arguments:
	# Ask the user for the models or files to convert
	user_input = input("Enter the model numbers or file paths to json data separated by a space: ")
	# Use shlex to slit the input to support quoted paths that contain spaces
	args.args.extend(shlex.split(user_input))

debug_arguments = False

if debug_arguments:
	out = f"""args.help = {args.help}
	args.json = {args.json}
	args.html = {args.html}
	args.md = {args.md}
	args.update = {args.update}
	args.print = {args.print}
	args.force_new_arguments = {args.force_new_arguments}
	args.args = {args.args}
	"""
	print(out)
	exit()

# Download an image and convert it to base64, unless it was previously already done so and saved in the image cache then just return the base64 stream
def convert_image_to_base64(image_url):
	global image_cache_changed
	if image_url is None or image_url == "":
		return ""

	cached_image = image_cache.get(image_url, None)

	# If the URL is in the cache, return the value
	if cached_image is not None:
		return cached_image

	# Check the secondary cache (which has a dictionary that points to a local path file)
	image_path = image_cache2.get(image_url, None)

	if image_path is not None:
		try:
			image = Image.open(image_path)
			buffer = BytesIO()
			format = image.format
			image.save(buffer, format=format)
			base64_image = base64.b64encode(buffer.getvalue()).decode()
			result = f"data:image/{format};base64,{base64_image}"
			image_cache[image_url] = result  # Add the URL and base64 to the cache
			image_cache_changed = True
			return result
		except:
			print("Failed to cache local image " + image_path)

	# Both caches do not contain the image so attempt to fetch it from the website
	response = requests.get(image_url)
	image_data = response.content
	try:
		image = Image.open(BytesIO(image_data))
		format = "JPEG" if image.format == "JPEG" else "PNG"
		base64_image = base64.b64encode(image_data).decode('utf-8')
		result = f"data:image/{format};base64,{base64_image}"
		image_cache[image_url] = result  # Add the URL and base64 to the cache
		image_cache_changed = True
		return result
	except Image.UnidentifiedImageError:
		if image_url not in image_cache.keys():
			image_cache[image_url] = None  # Add the URL with a None value to the cache
			image_cache_changed = True
	return image_url

def img_url(x):
	return "https://imagecache.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/" + x + "/width=400"

def img_md(image):
	# Return an empty string if the image object is None
	if image is None:
		return ""

	out = f"->![image]({img_url(image['url'])})<-\n"
	if image['meta'] is not None:
		meta_tags = list(image['meta'].keys())
		if meta_tags:
			# Add the image meta data to the output string
			out += "``` text\n"
			if "prompt" in meta_tags:
				out += f"{image['meta']['prompt']}\n"
			if "negativePrompt" in meta_tags:
				out += f"Negative prompt: {image['meta']['negativePrompt']}\n"
			for i, tag in enumerate(meta_tags):
				if tag == "cfgScale":
					# Add the cfgScale meta data to the output string
					out += f"CFG scale: {image['meta']['cfgScale']}, "
				elif tag != "prompt" and tag != "negativePrompt":
					# Add the other meta data to the output string, convert the tag to Proper case
					out += re.sub(r'\b\w', lambda x: x.group(0).upper(), tag, count=1) + ": " + str(image['meta'][tag]) + ", "
			# Remove any trailing commas or whitespace
			out = out.rstrip(", ")
			out += "\n```\n"
	return out
	
def img_html(image):
	# Return an empty string if the image object is None
	if image is None:
		return ""

	out = f'<div class="img-container"><img src="{convert_image_to_base64(img_url(image["url"]))}" />'
	if image['meta'] is not None:
		meta_tags = list(image['meta'].keys())
		if meta_tags:
			# Add the image meta data to the output string
			out += '<div class="img-meta-ico" title="Copy Metadata"></div><textarea class="img-meta">'
			meta_out = ""
			if "prompt" in meta_tags:
				meta_out += f"{image['meta']['prompt']}\n"
			if "negativePrompt" in meta_tags:
				meta_out += f"Negative prompt: {image['meta']['negativePrompt']}\n"
			for i, tag in enumerate(meta_tags):
				if tag == "cfgScale":
					# Add the cfgScale meta data to the output string
					meta_out += f"CFG scale: {image['meta']['cfgScale']}, "
				elif tag != "prompt" and tag != "negativePrompt":
					# Add the other meta data to the output string, convert the tag to Proper case
					meta_out += re.sub(r'\b\w', lambda x: x.group(0).upper(), tag, count=1) + ": " + str(image['meta'][tag]) + ", "
			# Remove any trailing commas or whitespace
			meta_out = meta_out.rstrip(", ")
			out += f"{meta_out}</textarea>"
	out += "</div>\n"
	return out

def generate_stars(rating, max_rating):
	# Initialize the output string with empty stars
	stars = ""

	# Add full stars for the whole rating
	for i in range(int(rating)):
		stars += "★"

	# Add a half star if the rating has a decimal
	if rating % 1 != 0:
		stars += "✭"

	# Add empty stars to fill up to the maximum rating
	for i in range(len(stars), max_rating):
		stars += "☆"

	return stars

def format_kb(kilobytes):
	# Convert kilobytes to megabytes
	megabytes = kilobytes / 1024

	# If the size is less than 1 megabyte, return the size in kilobytes
	if megabytes < 1:
		return f"{kilobytes:.1f}KB"

	# Convert megabytes to gigabytes
	gigabytes = megabytes / 1024

	# If the size is less than 1 gigabyte, return the size in megabytes
	if gigabytes < 1:
		return f"{megabytes:.1f}MB"

	# Return the size in gigabytes
	return f"{gigabytes:.1f}GB"

def format_date(date_string):
	# Create a new date object from the date string
	date = datetime.strptime(date_string, "%Y-%m-%dT%H:%M:%S.%fZ")

	# Format the date using the strftime() method
	formatted_date = date.strftime("%b %d, %Y")

	return formatted_date

def sanitize_filename(filename, max_length):
	# Remove single quotes
	filename = re.sub(r"'", "", filename)
	# Replace any non-alphanumeric characters with spaces and trim leading/trailing spaces
	filename = re.sub(r'[^a-zA-Z0-9]', ' ', filename).strip().rstrip(' ').title()
	# remove spaces
	filename = re.sub(r' +', '', filename)
	# If the resulting file name is empty or consists of only spaces, change it to 'default'
	if not filename or re.fullmatch(r' +', filename):
		filename = 'default'
	# Truncate the file name if it is longer than the provided maximum length
	if len(filename) > max_length:
		filename = filename[:max_length]
	return filename
	
def html_to_markdown(HTML):
	if HTML:
		HTML = markdownify.markdownify(HTML).strip()
	return HTML

def get_file_hash(file):
	if file['hashes'] and file['hashes'] is not None and len(file['hashes']) > 0:
		for hash in file['hashes']:
			if hash['type'] and hash['type'] is not None and hash['type'] == "AutoV1":
				return hash['hash']
	return None

def generate_md_file(data, reviews):
	out = ""
	# Add the favorite and rating count to the output string
	out += f"==🤍 {data['rank']['favoriteCountAllTime']}== =={generate_stars(data['rank']['ratingAllTime'], 5)} {data['rank']['ratingCountAllTime']}==\n"
	# Add the model name as a header to the output string
	out += f"# {data['name']}\n"
	# Add the model type to the output string
	out += f"->`{data['type']}`->\n"
	# Add the first image to the output string
	out += img_md(data['modelVersions'][0]['images'][0].get('image', data['modelVersions'][0]['images'][0]))
	# Remove HTML tags from the description and add it to the output string
	out += f"{html_to_markdown(data['description'])}\n\n"
	# Loop through the model versions
	for i in range(len(data['modelVersions'])):
		# Get the current model version
		model_version = data['modelVersions'][i]
		# Add a section heading for the model version
		out += "!!! info\n"
		# Add a link to download the model version to the output string
		# Loop through the files for this model
		for j in range(len(model_version['files'])):
			download_url = f"https://civitai.com/api/download/models/{model_version['id']}?type={model_version['files'][j]['type']}&format={model_version['files'][j]['format']}"
			# Add any other files to the output
			if j == 0:
				out += f"    **[Download ({format_kb(model_version['files'][j]['sizeKB'])})]({download_url})**"
			elif model_version['files'][j]['type'] == 'Model' or model_version['files'][j]['type'] == 'Pruned Model':
				pruned = ""
				if model_version['files'][j]['type'] == 'Pruned Model':
					pruned = " Pruned"
				out += f"** | [{model_version['files'][j]['format']}{pruned}]({download_url})**"
			else:
				out += f"** | [{model_version['files'][j]['type']}]({download_url})**"

		out += "\n"
		# Add the model version name and other data to the output string
		out += f"Version|{model_version['name']}\n"
		out += "-|-\n"
		out += f"Rating|{generate_stars(model_version['rank']['ratingAllTime'], 5)} ({model_version['rank']['ratingCountAllTime']})\n"
		out += f"Downloads|{model_version['rank']['downloadCountAllTime']}\n"
		out += f"Uploaded|{format_date(model_version['createdAt'])}\n"
		if model_version['trainedWords'] and len(model_version['trainedWords']) > 0:
			# Concatenate all trained words into a string and wrap them with tilda (`) characters separating with commas
			out += f"Trigger Words| `{'`, `'.join(model_version['trainedWords'])}`\n"
		if model_version['baseModel'] and model_version['baseModel'] is not None:
			out += f"Base Model|{model_version['baseModel']}\n"
		out += f"Format|{model_version['files'][0]['format']}\n"
		if model_version['description']:
			# Convert the description to markdown and check again to see if the new string is empty before adding to the output
			description = html_to_markdown(model_version['description'])
			if description and description != "":
				out += "!!! note About this version\n"
				out += f"    {description}\n"
		out += "\n"
		# Loop through the images for this version
		for j in range(len(model_version['images'])):
			# If this is the first model version and the first image we already used it at the top, so skip it
			if i == 0 and j == 0:
				continue
			# Add the current image to the output string
			out += img_md(model_version['images'][j].get('image', model_version['images'][j]))
		# Add a horizontal rule to separate the model versions from the rest of the document
		out += "***\n"
		# Add a section to the markdown doc for reviews, if there are any
		if len(reviews) > 0:
			out += "#### Discussion\n\n"
			# Iterate through the reviews
			for i in range(len(reviews)):
				review = reviews[i]
				# Only include the review/comment if it has an image or text
				if ("text" not in review or review["text"] is None) and ("content" not in review or review["content"] is None) and ("images" not in review or len(review["images"]) == 0):
					continue
				# Add the username as a subheading
				out += f"###### {review['user']['username']}\n"
				if "rating" in review and review["rating"] is not None:
					out += f'-->=={generate_stars(review["rating"], 5)}==-->\n'
				# Add the review text, if it exists
				if "text" in review and review["text"] is not None:
					out += "!!! note\n"
					out += f"    {html_to_markdown(review['text'])}\n"
				elif "content" in review and review["content"] is not None:
					out += "!!! note\n"
					out += f"    {html_to_markdown(review['content'])}\n"
				# Add the images for the review if they exist (it might be just a comment)
				if "images" in review and review["images"] is not None:
					for j in range(len(review["images"])):
						if review["images"][j].get("skip", False):
							continue
						out += img_md(review["images"][j])
				# Add a horizontal rule to separate the reviews
				out += "***\n"
	# Include these links just to have handy, they will not appear in the markdown preview because their hypertext is empty
	out += "->[​](https://rentry.org/)[​](https://ghostarchive.org/)<-\n"
	# Add a section for the source and archive links, hide the archive link by default. Remove the wrapping [​]() to make visible
	out += "-> *[source](https://civitai.com/models/{data['id']})*[​]( | *[archive](xxxxx)*) <-\n"
	return out
	
def generate_html_file(data, reviews):
	out = f'<!DOCTYPE html>\n<html>\n<head>\n'
	out += f'<title>{data["name"]}</title>\n'
	out += f'<meta http-equiv="content-type" content="text/html; charset=UTF-8" />\n'
	out += f'<meta name="viewport" content="width=device-width, initial-scale=1" />\n'
	out += '''<style>
	a {
		color: #1564df;
	}
	.page-container {
		position: relative;
		max-width: 1600px;
		margin: auto;
	}
	.page-heading {
		display: flex;
		flex-direction: row;
		flex-wrap: wrap;
		align-items: flex-start;
		gap: 10px;
		justify-content: space-between;
	}
	.page-heading h1 {
		flex: 1 0 100%;
	}
	.page-heading > div:last-of-type {
		flex: 0;
	}
	.likes-and-ranks {
		gap: 10px;
		display: flex;
		flex: 0;
	}
	.likes-and-ranks span {
		background: #eee;
		padding: 5px;
		white-space: nowrap;
	}
	h1 {
		text-align: center;
	}
	.type {
		background: rgb(231, 245, 255);
		color: rgb(34, 139, 230);
		padding: 5px;
		font-weight: bold;
		border-radius: 5px;
	}
	body {
		font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Helvetica, Arial, sans-serif, Apple Color Emoji, Segoe UI Emoji;
	}
	.img-meta {
		display: none;
	}
	.img-meta-ico::before {
		content: 'ⓘ';
		color: white;
		text-shadow: 1px 1px 1px black;
		padding: 0 6px;
		font-size: 22px;
		cursor: pointer;
		background: red;
		border-radius: 5px;
		display: grid;
		justify-content: center;
		align-content: center;
		overflow: inherit;
		position: absolute;
		top: 0;
		right: 0;
	}
	.img-meta-ico {
		display: inline-block;
		opacity: 0;
		transition: opacity 0.3s;
	}
	.img-container:hover .img-meta-ico {
		opacity: 1;
	}
	.img-set-container {
		display: grid;
		gap: 10px;
		grid-template-columns: min-content;
		grid-template-rows: min-content;
		align-content: center;
		align-items: stretch;
		overflow: auto;
		width: min-content;
		margin: auto;
		max-width: 100%;
	}
	.img-container {
		position: relative;
		border: 1px solid grey;
		width: 193px;
		height: 194px;
		overflow: hidden;
		background: #333;
	}
	.img-container:first-of-type,
	.version-img-set .img-container:nth-of-type(6),
	.version-img-set .img-container:nth-of-type(11),
	.version-img-set .img-container:nth-of-type(16)
	{
		width: 400px;
		height: 400px;
	}
	.comment .img-container:first-of-type,
	.comment .img-container:nth-of-type(4) {
		width: 400px;
		height: 400px;
	}
	.version-img-set .img-container:first-of-type {
		grid-area: 1 / 1 / 3 / 3;
	}
	.version-img-set .img-container:nth-of-type(6) {
		grid-area: 2 / 3 / 4 / 5;
	}
	.version-img-set .img-container:nth-of-type(11)
	{
		grid-area: 4 / 2 / 6 / 4;
	}
	.version-img-set .img-container:nth-of-type(16)
	{
		grid-area: 6 / 1 / 8 / 3;
	}
	.comment .img-container:first-of-type {
		grid-area: 1 / 1 / 1 / 3;
	}
	.comment .img-container:nth-of-type(4) {
		grid-area: 3 / 1 / 4 / 3
	}
	.img-container img {
		object-fit: contain;
		height: 100%;
		width: 100%;
		object-position: center;
		cursor: zoom-in;
	}
	.download {
		list-style: none;
		background: rgb(231, 245, 255);
		color: rgb(34, 139, 230);
		padding: 15px 20px;
		font-weight: bold;
		border-radius: 5px;
		flex: 1 100%;
	}
	.download li {
		display: inline-block;
	}
	.download li:not(:first-of-type)::before {
		content: "|";
		margin: 0 1em;
	}
	.download a {
		color: rgb(34, 139, 230);
	}
	table {
		border-spacing: 0;
		float: right;
		overflow: hidden;
		border: 1px solid grey;
		border-radius: 5px;
		margin-bottom: 1em;
		flex: 1;
	}
	th, td {
		text-align: left;
		padding: 10px;
		
	}
	tr:not(:first-of-type) > th, tr:not(:first-of-type) > td {
		border-top: 1px solid grey;
	}
	th {
		border-right: 1px solid grey;
		background-color: #d7d7d7;
		white-space: nowrap;
	}
	.tag {
		background: rgba(243, 240, 255, 1);
		color: #7950f2;
		border-radius: 5px;
		padding: 3px 8px;
		white-space: nowrap;
		cursor: pointer;
	}
	.tag-container {
		display: flex;
		flex-wrap: wrap;
		gap: 3px 8px;
	}
	.version-container {
		display: flex;
		flex-wrap: wrap;
		gap: 10px;
		align-items: start;
		flex-direction: row-reverse;
	}
	.version-container > hr {
		flex: 1 100%;
	}
	.version-description {
		flex: 1 0 33%;
		display: flex;
		flex-direction: column;
	}
	h4 {
		font-size: 130%;
	}
	.comment:only-of-type {
		grid-area: 1 / 1 / 1 / 4;
	}
	.comment {
		margin: auto;
		box-shadow: 2px 2px 10px #ccc;
		border: 1px solid #ccc;
		padding: 0 15px 15px;
		border-radius: 5px;
		background: #dce7f2;
		width: 850px;
		max-width: 100%;
		position: relative;
		box-sizing:border-box;
	}
	.comment:nth-last-of-type(2n) {
		background: #c5dbf2
	}
	.comment .comment-heading > h5 {
		color: #1b94dd;
		margin: 0;
	}
	.comment-heading {
		display: flex;
		justify-content: space-between;
		align-items: center;
		flex-wrap: wrap;
		gap: 5px;
		margin: 1em 0;
	}
	h5 {
		font-size: 110%;
	}
	.comment-text {
		padding-bottom: 1em;
	}
	.reviews {
		display: grid;
		flex-wrap: wrap;
		align-items: flex-start;
		justify-content: stretch;
		align-content: stretch;
		flex-direction: column;
		gap: 1em;
		max-width: 100%;
		overflow: hidden;
		grid-template-columns: repeat(auto-fill, 505px);
		justify-content: center;
		align-items: flex-start;
		grid-template-columns: repeat(auto-fill, minmax(420px, 1fr));
		grid-template-rows: masonry;
	}
	.comment .img-set-container {
		overflow-y: auto;
		align-items: stretch;
		flex: 1 0 380px;
		box-sizing: border-box;
		margin-top: 15px;
		position: relative;
		scroll-snap-type: y mandatory;
	}
	.comment .img-container {
		flex: 1;
		box-sizing: border-box;
		max-width: 100%;
		max-height: 100%;
		display: flex;
		position: relative;
		scroll-snap-align: start;
	}
	.comment .img-container img {
		flex: 1 1 min-content;
		background: black;
		object-fit: contain;
		object-position: center;
	}
	.source {
		padding: 10px;
		text-align: center;
	}
	.source a {
		color: #55f;
		text-decoration: none;
		font-style: italic;
	}
	.version-img-set {
		flex: 1 0 min-content;
		max-width: 100%;
	}
	hr {
		opacity: 0.2;
	}
	/* tab related css */
	.version-container:not(.active) {
		display: none;
	}
	/* Style the tab */
	.version-tabs {
		overflow: hidden;
	}
	/* Style the heading of the tabs */
	.version-tabs h1 {
		font-size: 100%;
		background-color: inherit;
		float: left;
		border: none;
		outline: none;
		padding: 0 16px;
		user-select: none;
	}
	/* Style the buttons that are used to open the tab content */
	.version-tabs button {
		background-color: inherit;
		float: left;
		border: none;
		outline: none;
		cursor: pointer;
		padding: 14px 16px;
		transition: 0.3s;
		border-bottom: 2px solid #ccc;
		border-radius: 4px 4px 0px 0px;
	}
	/* Change background color of buttons on hover */
	.version-tabs button:hover {
		background-color: rgb(248, 249, 250);
	}
	/* Create an active/current tablink class */
	.version-tabs button.active {
		border-color: rgb(34, 139, 230);
		background-color: rgb(248, 249, 250);
	}
	/* Zoom Styling */
	#overlay {
		position: fixed;
		top: 0;
		left: 0;
		width: 100%;
		height: 100%;
		background: rgba(0, 0, 0, 0.9);
		z-index: 999;
		visibility: hidden;
		opacity: 0;
		transition: all 0.2s ease-in-out;
		cursor: zoom-out;
	}
	#overlay.visible {
		visibility: visible;
		opacity: 1;
	}
	#overlay img {
		width: 90%;
		height: 90%;
		position: absolute;
		top: 50%;
		left: 50%;
		transform: translate(-50%, -50%);
		object-fit: contain;
	}
	</style>
	'''
	out += f'</head>\n<body>\n<div class="page-container"><div class="page-heading">'
	out += f'<div class="likes-and-ranks"><span>🤍 {data["rank"]["favoriteCountAllTime"]}</span><span>{generate_stars(data["rank"]["ratingAllTime"], 5)} {data["rank"]["ratingCountAllTime"]}</span></div>\n'
	out += f'<div class="type">{data["type"]}</div>\n'
	out += f'<h1>{data["name"]}</h1></div>\n'
	out += f'<div class="img-set-container">'
	out += img_html(data["modelVersions"][0]["images"][0].get("image", data["modelVersions"][0]["images"][0]))
	out += f'</div>\n'
	out += f'<div class="description">{data["description"]}</div>\n\n'
	out += '<div class="version-tabs">\n<h1>Versions:</h1>'
	versions_out = ''
	for i, model_version in enumerate(data["modelVersions"]):
		out += f'<button version-id="model-version-{model_version["id"]}"' + ( ' class="active"' if i == 0 else "" ) + f'>{model_version["name"]}</button>\n'
		versions_out += f'<div id="model-version-{model_version["id"]}" class="version-container{ " active" if i == 0 else "" }">\n'
		versions_out += f'<ul class="download">\n'
		for j in range(len(model_version["files"])):
			hash = get_file_hash(model_version["files"][j])
			hash_tag = ""
			if hash is not None:
				hash_tag = f'title="Hash: {hash}" '
			if j == 0:
				tag = f'Download ({format_kb(model_version["files"][0]["sizeKB"])})'
			elif model_version['files'][j]['type'] == 'Model' or model_version['files'][j]['type'] == 'Pruned Model':
				pruned = ""
				if model_version['files'][j]['type'] == 'Pruned Model':
					pruned = " Pruned"
				tag = model_version["files"][j]["format"] + pruned
			else:
				tag = model_version["files"][j]["type"]
			download_url = f'https://civitai.com/api/download/models/{model_version["id"]}?type={model_version["files"][j]["type"]}&format={model_version["files"][j]["format"]}'
			versions_out += f'<li><a href="{download_url}" {hash_tag} target="_blank" rel="noopener noreferrer">{tag}</a></li>\n'
		versions_out += f'</ul>\n'
		
		versions_out += f'<div class="version-description"><table>\n'
		versions_out += f'<tr><th>Rating</th><td>{generate_stars(model_version["rank"]["ratingAllTime"], 5)} ({model_version["rank"]["ratingCountAllTime"]})</td></tr>\n'
		versions_out += f'<tr><th>Downloads</th><td>{model_version["rank"]["downloadCountAllTime"]}</td></tr>\n'
		versions_out += f'<tr><th>Uploaded</th><td>{format_date(model_version["createdAt"])}</td></tr>\n'
		if model_version["trainedWords"] and len(model_version["trainedWords"]) > 0:
			versions_out += '<tr><th>Trigger Words</th><td><div class="tag-container">'
			for word in model_version["trainedWords"]:
				versions_out += '<span class="tag">'
				versions_out += word
				versions_out += ' <span class="tag-copy-ico"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><rect x="8" y="8" width="12" height="12" rx="2"></rect><path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path></svg></span>'
				versions_out += '</span>'
			versions_out += '</div></td></tr>\n'
		if model_version["baseModel"] and model_version["baseModel"] is not None:
			versions_out += f'<tr><th>Base Model</th><td>{model_version["baseModel"]}</td></tr>\n'
		versions_out += f'<tr><th>Format</th><td>{model_version["files"][0]["format"]}</td></tr>\n'
		versions_out += f'</table>\n'
		if model_version["description"] and model_version["description"] != "":
			versions_out += f'<div class="version-description-text">{model_version["description"]}</div>\n'
		versions_out += '</div>'

		versions_out += f'<div class="version-img-set"><div class="img-set-container">'
		for j, image in enumerate(model_version["images"]):
			if i == 0 and j == 0:
				continue
			versions_out += img_html(image.get("image", image))
		versions_out += f'</div></div>\n'

		versions_out += f'\n</div>'
	out += '</div>\n'
	out += versions_out

	# Add a section to the markdown doc for reviews, if there are any
	if len(reviews) > 0:
		out += f'<h4>Discussion</h4>\n<div class="reviews">'
		# Iterate through the reviews
		for i in range(len(reviews)):
			review = reviews[i]
			# Only include the review/comment if it has an image or text
			if ("text" not in review or review["text"] is None) and ("content" not in review or review["content"] is None) and ("images" not in review or len(review["images"]) == 0):
				continue
			# Get the create time
			date = datetime.strptime(review["createdAt"], "%Y-%m-%dT%H:%M:%S.%fZ")
			# Convert the datetime object to a timestamp (integer)
			timestamp = date.timestamp()
			# Add the username as a subheading
			out += f'<div class="comment" style="order: {int(timestamp)*-1}"><div class="comment-heading"><h5>{review["user"]["username"]}</h5>\n'
			if "rating" in review and review["rating"] is not None:
				out += f'<div class="comment-rating">{generate_stars(review["rating"], 5)}</div>'
			out += f'</div>\n'
			# Add the review text, if it exists
			if "text" in review and review["text"] is not None:
				out += f'<div class="comment-text">{review["text"]}</div>\n'
			elif "content" in review and review["content"] is not None:
				out += f'<div class="comment-text">{review["content"]}</div>\n'
			# Add the images for the review if they exist (it might be just a comment)
			if "images" in review and review["images"] is not None and len(review["images"]) > 0:
				out += f'<div class="img-set-container">'
				for j in range(len(review["images"])):
					if review["images"][j].get("skip", False):
						continue
					out += img_html(review["images"][j])
				out += f'</div>'
			out += "</div>\n"
		out += "</div>\n"
	# Include these links just to have handy, they will not appear in the markdown preview because their hypertext is empty
	out += "<!-- https://rentry.org/ https://ghostarchive.org/ -->"
	# Add a section for the source and archive links, hide the archive link by default. Remove the wrapping [​]() to make visible
	out += f'<div class="source"><a href="https://civitai.com/models/{data["id"]}" target="_blank" rel="noopener noreferrer">Source</a></div>\n'
	out += "</div>\n"
	out += """<script>
	const images = document.querySelectorAll(".img-container img");
	const overlay = document.createElement("div");
	overlay.setAttribute("id", "overlay");
	document.body.appendChild(overlay);

	images.forEach(image => {
		image.addEventListener("click", function() {
			overlay.innerHTML = `<img src="${image.src}">`;
			overlay.classList.add("visible");
		});
	});

	overlay.addEventListener("click", function() {
		overlay.classList.remove("visible");
	});

	document.querySelectorAll('.version-tabs button').forEach(button => {
		button.addEventListener('click', event => {
			document.querySelectorAll('.version-container').forEach(version => {
				version.classList.remove("active");
			});			
			document.querySelectorAll('.version-tabs button').forEach(btn => {
				btn.classList.remove("active");
			});
			event.target.classList.add("active");
			const versionAttr = event.target.getAttribute("version-id");

			let version = versionAttr.split("-").pop();
			history.replaceState(null, null, "#" + version);

			const versionDiv = document.getElementById(versionAttr);
			versionDiv.classList.add("active");
		});
	});

	document.querySelectorAll('.tag-container .tag').forEach(tag => {
		tag.addEventListener('click', event => {
			navigator.clipboard.writeText(event.target.textContent.trim());
		});
	});

	document.querySelectorAll('.img-meta-ico').forEach(ico => {
		ico.addEventListener('click', event => {
			const textarea = event.target.nextElementSibling;
			navigator.clipboard.writeText(textarea.value);
		});
	});

	document.addEventListener("DOMContentLoaded", function(event) { 
		if( window.location.hash ) {
			const hash = window.location.hash.substring(1);
			const activeTab = document.querySelector(`button[version-id="model-version-${hash}"]`)
			if (activeTab){
				activeTab.click()
			}
		}
	});
	</script>
	"""
	# Convert the dictionary to a JSON string
	json_data = json.dumps(rebuildSourceJSON(data, reviews), indent=2)
	out += f'<script type="application/json">{json_data}</script>'
	out += "</body>\n</html>"
	return out

def getPageDataFromModelData(modelData):
	# Get the main page json data
	pageData = modelData["pageData"]

	# get the relavent data from json
	return pageData["props"]["pageProps"]["trpcState"]["json"]["queries"][0]["state"]["data"]

def getReviewsFromModelData(modelData):
	# Get the main reviews json data
	reviewsJson = modelData["reviewData"]

	# hold the relavent review data
	reviews = []

	# Loop through the JSON array
	for i in range(len(reviewsJson)):
		# Get the reviews from the current object (Doing it this way incase more than one array object has reviews in it)
		reviewsTemp = reviewsJson[i].get('result', {}).get('data', {}).get('json', {}).get('reviews', None)
		if reviewsTemp:
			# Concatenate the reviews onto the main reviews array
			reviews += reviewsTemp
		else:
			# Check for comments without images (Doing it this way incase more than one array object has comments in it)
			commentsTemp = reviewsJson[i].get('result', {}).get("data", {}).get("json", {}).get("comments", [])
			if commentsTemp:
				# Concatenate the comments onto the main reviews array
				reviews += commentsTemp
	
	return reviews

def rebuildReviewsJson(reviews):
	reviewsJson = []
	comments = []
	for review in reviews:
		if "rating" in review:
			# This is a review
			reviewsJson.append(review)
		else:
			# This is a comment
			comments.append(review)
	return [
		{ "result": { "data": { "json": { "reviews": reviewsJson } } } },
		{ "result": { "data": { "json": { "comments": comments } } } }
	]

def rebuildSourceJSON(data, reviews):
	out = {}
	page_data = {
		"props": {
			"pageProps": {
				"trpcState": {
					"json": {
						"queries": [
							{
								"state": {
									"data": data
								}
							}
						]
					}
				}
			}
		}
	}
	out[data["id"]] = {
		"pageData": page_data,
		"reviewData": rebuildReviewsJson(reviews)
	}
	return out

def get_filename_for_url(url):
	if not url:
		return None

	headers = {'Range': 'bytes=0-0'}
	response = requests.get(url, headers=headers, allow_redirects=True)

	cd = response.headers.get('content-disposition')

	if not cd:
		return None

	fname = re.findall('filename=(.+)', cd)

	if len(fname) == 0:
		return None

	fname = fname[0].strip('"')

	if len(fname) == 0:
		return None

	return fname

def generate_version_files_array(data):
	extensions = [".ckpt",".safetensors",".pt",".bin"]
	sanitized_name = sanitize_filename(data['name'], file_name_max_size)
	html = f"{sanitized_name}_({data['id']}).html"
	folder_name = get_folder(data['type'])
	
	for i, model_version in enumerate(data["modelVersions"]):
		update_version_files = False
		files = []
		hashes = []
		version_names = []
		id = str(model_version['id'])
		sanitized_version_name = sanitize_filename(model_version['name'], file_name_max_size)
		for j in range(len(model_version["files"])):
			file_name = model_version["files"][j]["name"]
			file_type = model_version['files'][j]['type']
			file_format = model_version['files'][j]['format']

			if any(file_name.lower().endswith(ext) for ext in extensions) and file_type.lower() != "vae":
				update_version_files = True
				
				if id in version_files.keys() and sanitized_name == version_files[id]['name'] and sanitized_version_name in version_files[id]['versions']:
					continue # odds that the filename has changed since the last time it was saved is low, skip this file

				if sanitized_version_name not in version_names:
					version_names.append(sanitized_version_name)

				download_url = f"https://civitai.com/api/download/models/{model_version['id']}?type={file_type}&format={file_format}"
				file_name_em = file_name[:file_name.rindex(".")] + "_em" + file_name[file_name.rindex("."):]
				
				if file_name not in files:
					files.append(file_name)

				if file_name_em not in files and data["type"].lower() == "textualinversion":
					files.append(file_name_em)

				url_file_name = get_filename_for_url(download_url)

				if url_file_name is not None and file_name != url_file_name and url_file_name not in files:
					url_file_name_em = url_file_name[:url_file_name.rindex(".")] + "_em" + url_file_name[url_file_name.rindex("."):]
					files.append(url_file_name)
					if data["type"] == "TextualInversion" and url_file_name_em not in files:
						files.append(url_file_name_em)

				hash = get_file_hash(model_version["files"][j])
				if hash is not None:
					hashes.append(hash)

		if update_version_files:
			update = {}
			update[id] = {
				'name': sanitized_name,
				'versions': version_names,
				'html': html,
				'hashes': hashes,
				'files': files,
				'folder': folder_name
			}

			if id in version_files.keys():
				version_files[id]['name'] = update[id]['name']
				version_files[id]['versions'].extend(update[id]['versions'])
				version_files[id]['files'].extend(update[id]['files'])
				version_files[id]['hashes'].extend(update[id]['hashes'])
				version_files[id]['html'] = update[id]['html']
				version_files[id]['folder'] = update[id]['folder']
			else:
				version_files.update(update)

def processJSONData(json_data):
	# Create empty lists for the md and html files
	md_files = []
	html_files = []
	json_files = []
	json_merge = {}
	key_name_pairs = []

	# Create temporary files for the md and html strings
	with tempfile.TemporaryDirectory() as temp_dir:
		# Loop over the keys in the json object
		for key in json_data:
			# Get the main page json data
			model_data = json_data[key]
			data = getPageDataFromModelData(model_data)
			reviews = getReviewsFromModelData(model_data)

			if len(json_data) > 1 and args.print:
				print(f"{data['name']} ({key})")

			# Generate the md and html strings
			if args.md:
				md_string = generate_md_file(data, reviews)
			if args.html:
				html_string = generate_html_file(data, reviews)

			if args.file_names:
				# Generate/update the version files cache
				generate_version_files_array(data)

			# Rebuild the json source files
			rebuilt_json = rebuildSourceJSON(data, reviews)
			json_merge.update(rebuilt_json)
			json_string = json.dumps(rebuilt_json, indent=2)

			folder_name = get_folder(data['type'])
			key_name_pairs.append([key, data['name']])
			
			md_path = os.path.join(temp_dir, f"{sanitize_filename(data['name'], file_name_max_size)}_({key}).md")
			html_path = os.path.join(temp_dir, f"{sanitize_filename(data['name'], file_name_max_size)}_({key}).html")
			json_path = os.path.join(temp_dir, f"{sanitize_filename(data['name'], file_name_max_size)}_({key}).json")

			if args.md:
				with open(md_path, 'w', encoding='utf-8') as f:
					f.write(md_string.encode('utf-8').decode('utf-8'))

			if args.html:
				with open(html_path, 'w', encoding='utf-8') as f:
					f.write(html_string.encode('utf-8').decode('utf-8'))

			if args.json:
				with open(json_path, 'w', encoding='utf-8') as f:
					f.write(json_string.encode('utf-8').decode('utf-8'))

			# Add the md and html files to the appropriate lists
			if args.md:
				md_files.append({'path': md_path, 'folder': folder_name})
			if args.html:
				html_files.append({'path': html_path, 'folder': folder_name})
			if args.json:
				json_files.append({'path': json_path, 'folder': folder_name})

		# Check if either of the lists has more than one file
		if len(md_files) > 1 or len(html_files) > 1 or len(json_files) > 1:
			# Create the zip files
			with zipfile.ZipFile('civitai.zip', 'w') as output_zip:
				outputfiles = []
				
				if args.md:
					outputfiles += md_files
				if args.html:
					outputfiles += html_files
				if args.json:
					outputfiles += json_files

				for file in outputfiles:
					arcname = file['folder'] + '/' + os.path.basename(file['path'])
					output_zip.write(file['path'], arcname=arcname)
				
				# Generate and save a markdown file that contains the arguments used when generating this zip and which models were generated
				sorted_pairs = sorted(key_name_pairs, key=lambda x: x[1].lower())
				joined_pairs = "\n".join(["|".join(pair) for pair in sorted_pairs])

				model_nums_str = f'# Arguments:\n'
				model_nums_str += f'``` text\n'
				model_nums_str += ' '.join(args.args)
				model_nums_str += f'\n```\n\n'
				model_nums_str += f'# Models:\n'
				model_nums_str += f'Key|Model\n'
				model_nums_str += f'-|-\n'
				model_nums_str += joined_pairs
				output_zip.writestr('model_nums.md', model_nums_str)

				# Write the merged source json file to the zip
				output_zip.writestr('merged.json', json.dumps(json_merge, indent=2))
		else:
			if args.md:
				save_files(md_files)
			if args.html:
				save_files(html_files)
			if args.json:
				save_files(json_files)

	if image_cache_changed:
		print("writing image cache")
		# Save the cache to a file
		with open('cache.json', 'w') as f:
			json.dump(image_cache, f)

	if args.file_names:
		# Save the version files cache to a file
		with open('version_files.json', 'w') as f:
			json.dump(version_files, f, indent=2)

def save_files(files):
	if len(files) == 1:
		# Get the file name and extension of the file
		file_name, file_ext = os.path.splitext(os.path.basename(files[0]['path']))
		folder_name = files[0]['folder']

		print(folder_name + "/" + file_name + file_ext)
			
		# Get the contents of the file
		with open(files[0]['path'], 'r', encoding='utf-8') as f:
			contents = f.read()
			
		# Check if the directory exists
		if not os.path.exists(folder_name):
			# If it doesn't exist, create the directory
			os.makedirs(folder_name)
			
		# Write the contents to a new file with the same name and extension as the input file
		with open(folder_name + '/' + file_name + file_ext, 'w', encoding='utf-8') as f:
			f.write(contents)


def update_data(old_data, new_data):
	# Iterate over the keys in the new data
	for key in new_data:
		# If the key exists in the old data, update the old data with the new data
		if key in old_data:
			# Update the pageData
			old_model_versions = old_data[key]['pageData']['props']['pageProps']['trpcState']['json']['queries'][0]['state']['data']['modelVersions']
			new_model_versions = new_data[key]['pageData']['props']['pageProps']['trpcState']['json']['queries'][0]['state']['data']['modelVersions']
			
			# check if new model version already exists in old data, if not, extend the array
			for new_version in new_model_versions:
				version_found = False
				for old_version in old_model_versions:
					if new_version['id'] == old_version['id']:
						version_found = True
						old_images = old_version.get('images', [])
						new_images = new_version.get('images', [])
						
						# extend images array and add any new images
						for new_image in new_images:
							image_found = False
							for old_image in old_images:
								if new_image['id'] == old_image['id']:
									image_found = True
									break
							if not image_found:
								old_images.append(new_image)
						old_version['images'] = old_images
						break
						
				if not version_found:
					old_model_versions.append(new_version)

			# update pageData with modified modelVersions array
			old_data[key]['pageData']['props']['pageProps']['trpcState']['json']['queries'][0]['state']['data']['modelVersions'] = old_model_versions
			
			# Update the comments
			old_data[key]['reviewData'][1] = new_data[key]['reviewData'][1]
			# Iterate over the list of reviews in the new data
			for new_review in new_data[key]['reviewData'][0]['result']['data']['json']['reviews']:
				# Check if the review already exists in the old data
				review_exists = False
				for old_review in old_data[key]['reviewData'][0]['result']['data']['json']['reviews']:
					if old_review['id'] == new_review['id']:
						review_exists = True
						break
				# If the review doesn't exist, append it to the old data's list of reviews
				if not review_exists:
					old_data[key]['reviewData'][0]['result']['data']['json']['reviews'].append(new_review)
				# If the review does exist, merge the images lists
				else:
					# Iterate over the list of images in the new review
					for new_image in new_review['images']:
						# Check if the image already exists in the old review's list of images
						image_exists = False
						for old_image in old_review['images']:
							if old_image['id'] == new_image['id']:
								image_exists = True
								break
						# If the image doesn't exist, append it to the old review's list of images
						if not image_exists:
							old_review['images'].append(new_image)
		# If the key doesn't exist in the old data, add it
		else:
			old_data[key] = new_data[key]

def getNewDataFromSiteForModel(model_num):
	# Make sure it's a valid model number
	if model_num and int(model_num) > 0:
		# Set the value of the url_model variable to the URL of the model page
		url_model = f"https://civitai.com/models/{model_num}/"
		# Set the value of the url_reviews variable to the URL of the review data
		url_reviews = f"https://civitai.com/api/trpc/review.getAll,comment.getAll?batch=1&input=%7B%220%22%3A%20%7B%22json%22%3A%20%7B%22modelId%22%3A%20{model_num}%2C%22limit%22%3A%20{review_limit}%2C%22sort%22%3A%20%22newest%22%2C%22cursor%22%3A%20null%7D%2C%22meta%22%3A%20%7B%22values%22%3A%20%7B%22cursor%22%3A%20%5B%22undefined%22%5D%7D%7D%7D%2C%221%22%3A%20%7B%22json%22%3A%20%7B%22modelId%22%3A%20{model_num}%2C%22limit%22%3A%20{comment_limit}%2C%22sort%22%3A%20%22newest%22%2C%22cursor%22%3A%20null%7D%2C%22meta%22%3A%7B%22values%22%3A%7B%22cursor%22%3A%5B%22undefined%22%5D%7D%7D%7D%7D"
		# Send a GET request to the model page and parse the response
		response = requests.get(url_model)

		if response.status_code == 200:
			doc = BeautifulSoup(response.text, "html.parser")
		else:
			print("Error: Invalid response from server")

		# Extract the page data from the response
		page_data = doc.find(id="__NEXT_DATA__").text

		# Parse the page data text as JSON
		page_data = json.loads(page_data)

		headers = {
			"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/110.0",
			"Accept": "*/*",
			"Accept-Language": "en-CA,en-US;q=0.7,en;q=0.3",
			"Accept-Encoding": "utf-8",
			"Referer": f"https://civitai.com/models/{model_num}",
			"content-type": "application/json",
			"DNT": "1",
			"Connection": "keep-alive",
			#"Cookie": "COOKIE HERE",
			"Sec-Fetch-Dest": "empty",
			"Sec-Fetch-Mode": "cors",
			"Sec-Fetch-Site": "same-origin",
			"TE": "trailers",
		}

		# Send a GET request to the review page and parse the response
		response = requests.get(url_reviews, headers=headers)
		review_data = response.json()
		# Add the data to the data object
		new_data = {}
		new_data[model_num] = {
			"pageData": page_data,
			"reviewData": review_data
		}

		# Sanitize the reviews/comments object by processing them and then un-processing them
		new_data[model_num]["reviewData"] = rebuildReviewsJson(getReviewsFromModelData(new_data[model_num]))

		return new_data


#################################################################################################################
################################################## SCRIPT #######################################################
#################################################################################################################

# Initialize the data object
data = {}

for arg in args.args:
	# If the value is a file path, load the JSON data from the file and add it to the data object
	if os.path.isfile(arg):
		with open(arg, 'r') as f:
			file_data = json.load(f)
		
		# Sanitize the reviews/comments object by processing them and then un-processing them
		for key in file_data:
			file_data[key]["reviewData"] = rebuildReviewsJson(getReviewsFromModelData(file_data[key]))
			
			if args.update:
				# Request the new data from the website for this model
				new_data = getNewDataFromSiteForModel(key)
				# Update the data dictionary with the new data
				update_data(file_data, new_data)
		
		# Update the data dictionary with the new data
		update_data(data, file_data)

	# Otherwise, assume it's a model number and send a GET request for the data
	else:
		# Request the new data from the website for this model
		new_data = getNewDataFromSiteForModel(arg)
		# Update the data dictionary with the new data
		update_data(data, new_data)

# Run the code to process the data
processJSONData(data)