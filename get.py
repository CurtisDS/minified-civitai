import re
import os
import sys
import json
import time
import shlex
import base64
import random
import argparse
import requests
import markdownify
import urllib.parse
from PIL import Image
from io import BytesIO
from datetime import datetime
from bs4 import BeautifulSoup

# get the start time
start_time = time.time()

# Declare variables
review_limit = 12
comment_limit = 4
file_name_max_size = 60

def get_folder(type):
	if type.lower() == "textualinversion":
		folder = "embeddings"
	elif type.lower() == "hypernetwork":
		folder = "hypernetwork"
	elif type.lower() == "checkpoint":
		folder = "models"
	elif type.lower() == "lora":
		folder = "lora"
	elif type.lower() == "locon":
		folder = "locon"
	else:
		folder = "unknownCivitai"
	return folder

class NewlineFormatter(argparse.RawDescriptionHelpFormatter):
	def _split_lines(self, text, width):
		return text.splitlines()

help_description = f'Get and parse Civitai model pages into HTML pages, markdown pages, and or save the source JSON from the website.\n\
This script takes the generated files for a single model and saves them in a folder depending on the type of model.\n\n\
Files will be saved in:\n\
  "./{get_folder("Checkpoint")}/" for Checkpoint\n\
  "./{get_folder("TextualInversion")}/" for TextualInversion\n\
  "./{get_folder("Hypernetwork")}/" for Hypernetwork\n\
  "./{get_folder("Lora")}/" for Lora\n\
  "./{get_folder("Locon")}/" for Locon\n\
'

help_epilog = f'This script can create a markdown (.md), HTML (.html), and JSON (.json) file for each model.\n\
You can control what files are output by setting flags for each file type you want generated by using command line arguments.\n\
If no output flags are set it will default to create every file type. You are able to use more than one output flag at once,\n\
but using all of them is equivalent to using none of them.'

parser = argparse.ArgumentParser(description=help_description, add_help=False, epilog=help_epilog, formatter_class=NewlineFormatter)
parser.add_argument('-c', '--combine', type=str, default='merged.json', help='file path to existing merged JSON (defaults to merged.json)')
parser.add_argument('-j', '--json', action='store_true', help='turn on JSON output flag')
parser.add_argument('-h', '--html', action='store_true', help='turn on HTML output flag')
parser.add_argument('-m', '--md', action='store_true', help='turn on Markdown output flag')
parser.add_argument('-l', '--md-legacy', action='store_true', help='if markdown output flag is on use legacy algorithm to generate the markdown')
parser.add_argument('-b', '--no-base64-images', action='store_true', help='use urls instead of base64 strings for images')
parser.add_argument('-n', '--file-names', action='store_true', help='turn on "version files" list output flag (a file that will associate the civitai id to its model name hash and html file)')
parser.add_argument('-u', '--update', action='store_true', help='update models found in provided json files. Otherwise models will have files regenerated using only the existing data from json file.')
parser.add_argument('-o', '--only-update-missing', action='store_true', help='do not download new data even when models are individually provided unless model data is not found in combined/merged json file (overrides --update).')
parser.add_argument('-f', '--force-new-arguments', action='store_true', help='on top of the initial arguments ask the user for new arguments once started (allows you to use a bat file to set default arguments but still add more when running)')
parser.add_argument('-p', '--print', action='store_true', help='turn on flag to print to console all model names and keys when processing multiple models at once')
parser.add_argument('-d', '--debug', action='store_true', help='turn on flag to print debug lines to console')
parser.add_argument('-?', '--help', action='store_true', help='show this help message and exit')
parser.add_argument('-rc', '--review-count', type=int, default=review_limit, help='how many reviews to download per model')
parser.add_argument('-cc', '--comment-count', type=int, default=comment_limit, help='how many comments to download per model')
parser.add_argument('args', nargs='*', help='model numbers or file paths to json files that contain previously downloaded data. If not specified you will be asked for this at runtime')
args = parser.parse_args()

if args.help:
	# Show the help message and exit
	parser.print_help()
	exit()

if args.only_update_missing:
	# Override the update setting
	args.update = False

if not any([args.json, args.html, args.md, args.file_names]):
	# Enable all output formats by default
	args.json = True
	args.html = True
	args.md = True
	args.file_names = True

if not args.args or args.force_new_arguments:
	# Ask the user for the models or files to convert
	user_input = input("Enter the model numbers or file paths to json data separated by a space: ")
	# Use shlex to slit the input to support quoted paths that contain spaces
	args.args.extend(shlex.split(user_input))

debug_arguments = False

if debug_arguments:
	out = f"""
    args.combine = {args.combine}
    args.help = {args.help}
    args.json = {args.json}
    args.html = {args.html}
    args.md = {args.md}
    args.md_legacy = {args.md_legacy}
    args.no_base64_images = {args.no_base64_images}
    args.file_names = {args.file_names}
    args.update = {args.update}
    args.only_update_missing = {args.only_update_missing}
    args.print = {args.print}
    args.debug = {args.debug}
    args.force_new_arguments = {args.force_new_arguments}
    args.args = {args.args}
    args.review_count = {args.review_count}
    args.comments_count = {args.comments_count}
	"""
	print(out)
	exit()

review_limit = args.review_count
comment_limit = args.comment_count

# Load the file names previously found from a file if it exists
if os.path.isfile('version_files.json'):
	with open('version_files.json', 'r') as f:
		version_files = json.load(f)
	f.close()
else:
	version_files = {}

# Load cookie file
if os.path.isfile('cookie.txt'):
	with open('cookie.txt', 'r') as f:
		cookie = f.read()
	f.close()
else:
	cookie = None

# split a string into segments of strings and ints that will be used to sort something naturally
def natural_order_number(s):
	return [int(x) if x.isdigit() else x.lower() for x in re.split('(\d+)', s)]

image_cache_set = {}

# Download an image and convert it to base64, unless it was previously already done so and saved in the image cache then just return the base64 stream or just the direct civitai url if no-base64-images is set
def image_to_url(image):
	image_key = get_image_key(image)
	if image_key not in image_cache_set:
		buffer = convert_image_to_base64(image_key)
		image_cache_set[image_key] = buffer
		return buffer
	else:
		return image_cache_set[image_key]

def get_image_key(image):
	return image['url'].replace("https://imagecache.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/", "").split("/")[0]

def is_same_image_id(new_image, old_image):
	# The new api doesn't have an image id attribute but it does include the id at the end of the url attribute.
	if 'id' not in new_image:
		new_image['id'] = int(new_image['url'].split("/")[-1])
	if 'id' not in old_image:
		old_image['id'] = int(old_image['url'].split("/")[-1])
	return new_image['id'] == old_image['id']

def convert_image_to_base64(image_key):
	if image_key is None or image_key == "":
		return ""

	image_url = f"https://imagecache.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/{image_key}/width=400"
	
	if args.no_base64_images:
		return image_url

	if args.debug:
		sys.stdout.write(f'Searching cache ({image_url})...')
		sys.stdout.flush()

	# Try to find and load the image from the image cache of files
	cache_file = os.path.join('image_cache', image_key)
	cache_exists = os.path.isfile(cache_file)
	if cache_exists:
		with open(cache_file, 'rb') as f:
			image_data = f.read()
		f.close()
	else:
		image_data = None

	# If the URL is in the cache, return the value
	if image_data is None:

		# Image is not cached so attempt to fetch it from the website
		if args.debug:
			sys.stdout.write(' Downloading image...')
			sys.stdout.flush()
		elif args.print and random.random() < 0.3:
			# only output this 1/3 of the time
			sys.stdout.write(f'.')
			sys.stdout.flush()
		try:
			response = requests.get(image_url)
			image_data = response.content
		except Exception as e:
			if args.debug:
				print('ED!')
				print(e)
			elif args.print:
				sys.stdout.write(f'!')
				sys.stdout.flush()
			return image_url

	# Convert the image to base64
	try:
		image = Image.open(BytesIO(image_data))
		base64_image = base64.b64encode(image_data).decode('utf-8')
	except Image.UnidentifiedImageError:
		if args.debug:
			print('EI!')
		elif args.print:
			sys.stdout.write(f'!')
			sys.stdout.flush()
		return image_url

	# Save the image to cache
	try:
		if image.format:
			image_format = image.format
		else:
			image_format = "PNG"
		if not cache_exists:
			image.save(cache_file, format=image_format)
	except Exception as e:
		if args.debug:
			print('ES!', image_format, e)
		elif args.print:
			sys.stdout.write(f'!')
			sys.stdout.flush()

	result = f"data:image/{image_format};base64,{base64_image}"
	if args.debug:
		print('!')
	return result

def img_md_legacy(image):
	# Return an empty string if the image object is None
	if image is None:
		return ""

	out = [f"->![image]({image_to_url(image)})<-\n"]
	if image['meta'] is not None:
		meta_tags = list(image['meta'].keys())
		if meta_tags:
			# Add the image meta data to the output string
			meta_out = ["``` text\n"]
			if "prompt" in meta_tags:
				meta_out.append(f"{image['meta']['prompt']}\n")
			if "negativePrompt" in meta_tags:
				meta_out.append(f"Negative prompt: {image['meta']['negativePrompt']}\n")
			for i, tag in enumerate(meta_tags):
				if tag == "cfgScale":
					# Add the cfgScale meta data to the output string
					meta_out.append(f"CFG scale: {image['meta']['cfgScale']}, ")
				elif tag != "prompt" and tag != "negativePrompt" and tag != "resources" and tag != "hashes":
					# Add the other meta data to the output string, convert the tag to Proper case
					meta_out.append(re.sub(r'\b\w', lambda x: x.group(0).upper(), tag, count=1) + ": " + str(image['meta'][tag]) + ", ")
			# Remove any trailing commas or whitespace
			out.append("".join(meta_out).rstrip(", ") + "\n```\n")
	return "".join(out)

def img_md(image):
	# Return an empty string if the image object is None
	if image is None:
		return ""

	out = ['<div style="display: inline-block; width: 410px; vertical-align: top; text-align: center; margin: 5px">']
	out.append(f'<img style="width: 400px; margin:auto" src="{image_to_url(image)}">')

	if image['meta'] is not None:
		meta_tags = list(image['meta'].keys())
		if meta_tags:
			# Add the image meta data to the output string
			meta_out = [f"<details>\n<summary>Metadata</summary>\n\n``` text\n"]
			if "prompt" in meta_tags:
				meta_out.append(f"{image['meta']['prompt']}\n")
			if "negativePrompt" in meta_tags:
				meta_out.append(f"Negative prompt: {image['meta']['negativePrompt']}\n")
			for i, tag in enumerate(meta_tags):
				if tag == "cfgScale":
					# Add the cfgScale meta data to the output string
					meta_out.append(f"CFG scale: {image['meta']['cfgScale']}, ")
				elif tag != "prompt" and tag != "negativePrompt" and tag != "resources" and tag != "hashes":
					# Add the other meta data to the output string, convert the tag to Proper case
					meta_out.append(re.sub(r'\b\w', lambda x: x.group(0).upper(), tag, count=1) + ": " + str(image['meta'][tag]) + ", ")
			# Remove any trailing commas or whitespace
			out.append("".join(meta_out).rstrip(", ") + "\n```\n</details>\n")
	out.append('</div>')
	return "".join(out)
	
def img_html(image):
	# Return an empty string if the image object is None
	if image is None:
		return ""

	out = [f'<div class="img-container"><img src="{image_to_url(image)}" />']
	if image['meta'] is not None:
		meta_tags = list(image['meta'].keys())
		if meta_tags:
			# Add the image meta data to the output string
			out.append('<div class="img-meta-ico" title="Copy Metadata"></div><textarea class="img-meta">')
			meta_out = []
			if "prompt" in meta_tags:
				meta_out.append(f"{image['meta']['prompt']}\n")
			if "negativePrompt" in meta_tags:
				meta_out.append(f"Negative prompt: {image['meta']['negativePrompt']}\n")
			for i, tag in enumerate(meta_tags):
				if tag == "cfgScale":
					# Add the cfgScale meta data to the output string
					meta_out.append(f"CFG scale: {image['meta']['cfgScale']}, ")
				elif tag != "prompt" and tag != "negativePrompt" and tag != "resources" and tag != "hashes":
					# Add the other meta data to the output string, convert the tag to Proper case
					meta_out.append(re.sub(r'\b\w', lambda x: x.group(0).upper(), tag, count=1) + ": " + str(image['meta'][tag]) + ", ")
			# Remove any trailing commas or whitespace
			meta_out_string = "".join(meta_out).rstrip(", ")
			out.append(f"{meta_out_string}</textarea>")
	out.append("</div>\n")
	return "".join(out)

def generate_stars(rating, max_rating):
	# Initialize the output string with empty stars
	stars = ""

	# Add full stars for the whole rating
	for i in range(int(rating)):
		stars += "★"

	# Add a half star if the rating has a decimal
	if rating % 1 != 0:
		stars += "✭"

	# Add empty stars to fill up to the maximum rating
	for i in range(len(stars), max_rating):
		stars += "☆"

	return stars

def format_kb(kilobytes):
	# Convert kilobytes to megabytes
	megabytes = kilobytes / 1024

	# If the size is less than 1 megabyte, return the size in kilobytes
	if megabytes < 1:
		return f"{kilobytes:.1f}KB"

	# Convert megabytes to gigabytes
	gigabytes = megabytes / 1024

	# If the size is less than 1 gigabyte, return the size in megabytes
	if gigabytes < 1:
		return f"{megabytes:.1f}MB"

	# Return the size in gigabytes
	return f"{gigabytes:.1f}GB"

def format_date(date_string):
	# Create a new date object from the date string
	date = datetime.strptime(date_string, "%Y-%m-%dT%H:%M:%S.%fZ")

	# Format the date using the strftime() method
	formatted_date = date.strftime("%b %d, %Y")

	return formatted_date

def sanitize_filename(filename, max_length):
	# Remove single quotes
	filename = re.sub(r"'", "", filename)
	# Replace any non-alphanumeric characters with spaces and trim leading/trailing spaces
	filename = re.sub(r'[^a-zA-Z0-9]', ' ', filename).strip().rstrip(' ').title()
	# remove spaces
	filename = re.sub(r' +', '', filename)
	# If the resulting file name is empty or consists of only spaces, change it to 'default'
	if not filename or re.fullmatch(r' +', filename):
		filename = 'default'
	# Truncate the file name if it is longer than the provided maximum length
	if len(filename) > max_length:
		filename = filename[:max_length]
	return filename
	
def html_to_markdown(HTML):
	if HTML:
		HTML = markdownify.markdownify(HTML).strip()
	return HTML

def get_file_hash(file, hash_type = "AutoV1"):
	if 'hashes' in file and file['hashes'] is not None and len(file['hashes']) > 0:
		if hash_type in file['hashes']:
			return file['hashes'][hash_type]
		for hash in file['hashes']:
			if 'type' in hash and hash['type'] is not None and hash['type'].lower() == hash_type.lower():
				return hash['hash']
	return None

def get_best_main_image(model_versions):
	second_best_image = None
	for model_version in model_versions:
		image = model_version["images"][0].get("image", model_version["images"][0])
		if not second_best_image:
			second_best_image = image
		if not model_version['name'].endswith(('inpainting', 'instruct-pix2pix')):
			return image
	return second_best_image

def generate_md_file_legacy(data, reviews):
	model_versions = sorted(data['modelVersions'], key=lambda x: x['createdAt'], reverse=True)
	out = []
	# Add the favorite and rating count to the output string
	out.append(f"==🤍 {data['rank']['favoriteCountAllTime']}== =={generate_stars(data['rank']['ratingAllTime'], 5)} {data['rank']['ratingCountAllTime']}==\n")
	# Add the model name as a header to the output string
	out.append(f"# {data['name']}\n")
	# Add the model type to the output string
	out.append(f"->`{data['type']}`->\n")
	# Add the first image to the output string
	out.append(img_md_legacy(get_best_main_image(model_versions)))
	# Remove HTML tags from the description and add it to the output string
	out.append(f"{html_to_markdown(data['description'])}\n\n")
	# Loop through the model versions
	for model_version in model_versions:
		# Add a section heading for the model version
		out.append("!!! info\n")
		# Add a link to download the model version to the output string
		# Loop through the files for this model
		for j, file in enumerate(model_version['files']):
			download_url = file.get('downloadUrl', f"https://civitai.com/api/download/models/{model_version['id']}?type={urllib.parse.quote(file['type'])}&format={urllib.parse.quote(file['format'])}")
			# Add any other files to the output
			if j == 0:
				out.append(f"    **[Download ({format_kb(file['sizeKB'])})]({download_url})**")
			elif file['type'] == 'Model' or file['type'] == 'Pruned Model':
				pruned = ""
				if file['type'] == 'Pruned Model':
					pruned = " Pruned"
				out.append(f"** | [{file['format']}{pruned}]({download_url})**")
			else:
				out.append(f"** | [{file['type']}]({download_url})**")

		out.append(f"\n\n#### Version {model_version['name']}\n")
		# Add the model version name and other data to the output string
		out.append(f"Version|{model_version['name']}\n")
		out.append("-|-\n")
		if 'rank' in model_version:
			out.append(f"Rating|{generate_stars(model_version['rank']['ratingAllTime'], 5)} ({model_version['rank']['ratingCountAllTime']})\n")
			out.append(f"Downloads|{model_version['rank']['downloadCountAllTime']}\n")
		out.append(f"Uploaded|{format_date(model_version['createdAt'])}\n")
		if model_version['trainedWords']:
			# Concatenate all trained words into a string and wrap them with tilda (`) characters separating with commas
			out.append(f"Trigger Words| `{'`, `'.join(model_version['trainedWords'])}`\n")
		if model_version['baseModel'] and model_version['baseModel'] is not None:
			out.append(f"Base Model|{model_version['baseModel']}\n")
		out.append(f"Format|{model_version['files'][0]['format']}\n")
		if model_version['description']:
			# Convert the description to markdown and check again to see if the new string is empty before adding to the output
			description = html_to_markdown(model_version['description'])
			if description and description != "":
				out.append("!!! note About this version\n")
				description_string = description.replace('\n', '\n    ')
				out.append(f"    {description_string}\n")
		out.append("\n")
		# Loop through the images for this version
		for j, model_version_image in enumerate(model_version['images']):
			# Add the current image to the output string
			out.append(img_md_legacy(model_version_image.get('image', model_version_image)))
		# Add a horizontal rule to separate the model versions from the rest of the document
		out.append("***\n")
	# Add a section to the markdown doc for reviews, if there are any
	if reviews:
		out.append("#### Discussion\n\n")
		# Iterate through the reviews
		for review in reviews:
			# Only include the review/comment if it has an image or text
			if ("text" not in review or review["text"] is None) and ("content" not in review or review["content"] is None) and ("images" not in review or len(review["images"]) == 0):
				continue
			# Add the username as a subheading
			out.append(f"###### {review['user']['username']}\n")
			if "rating" in review and review["rating"] is not None:
				out.append(f'->=={generate_stars(review["rating"], 5)}==->\n')
			# Add the review text, if it exists
			review_text_string = None
			if "text" in review and review["text"] is not None:
				review_text_string = review['text']
			elif "content" in review and review["content"] is not None:
				review_text_string = review['content']
			if review_text_string:
				review_text_string = html_to_markdown(review_text_string).strip()
				if review_text_string != "":
					review_text_string = review_text_string.replace('\n', '\n    ')
					out.append("!!! note\n")
					out.append(f"    {review_text_string}\n")
			# Add the images for the review if they exist (it might be just a comment)
			if "images" in review and review["images"] is not None:
				for review_image in review["images"]:
					if review_image.get("skip", False):
						continue
					out.append(img_md_legacy(review_image))
			# Add a horizontal rule to separate the reviews
			out.append("***\n")
	# Include these links just to have handy, they will not appear in the markdown preview because their hypertext is empty
	out.append("->[​](https://rentry.org/)[​](https://ghostarchive.org/)<-\n")
	# Add a section for the source and archive links, hide the archive link by default. Remove the wrapping [​]() to make visible
	out.append(f"-> *[source](https://civitai.com/models/{data['id']})*[​]( | *[archive](xxxxx)*) <-\n")
	return "".join(out)


def generate_md_file(data, reviews):
	model_versions = sorted(data['modelVersions'], key=lambda x: x['createdAt'], reverse=True)
	out = []
	# Add the favorite, rating, and type count to the output string
	out.append(f"<span style='background:#eee'>🤍 {data['rank']['favoriteCountAllTime']}</span> <span style='background:#eee'>{generate_stars(data['rank']['ratingAllTime'], 5)} {data['rank']['ratingCountAllTime']}</span> <span style='margin-right: 7px; float: right; background: rgb(231, 245, 255); color: rgb(34, 139, 230); padding: 5px; font-weight: bold; border-radius: 5px;'>{data['type']}</span>\n")
	# Add the model name as a header to the output string
	out.append(f"#  <div style='text-align: center'>{data['name']}</div>\n")
	# Add the first image to the output string
	out.append(f"<div style='text-align: center'>\n{img_md(get_best_main_image(model_versions))}\n</div>\n")
	# Remove HTML tags from the description and add it to the output string
	out.append(f"{html_to_markdown(data['description'])}\n\n")
	first_version = True
	trigger_words_style = "background: rgba(243, 240, 255, 1); color: #7950f2; border-radius: 5px; padding: 3px 8px; white-space: nowrap;"
	out.append(f"## Versions\n")
	# Loop through the model versions
	for model_version in model_versions:
		# Add a section heading for the model version
		out.append(f"<details{' open' if first_version else ''}>\n<summary>{model_version['name']}</summary>\n\n")
		out.append(f"<div style='background: rgb(231, 245, 255); padding: 5px 15px;'>\n\n")
		# Add a link to download the model version to the output string
		# Loop through the files for this model
		for j, file in enumerate(model_version['files']):
			download_url = file.get('downloadUrl', f"https://civitai.com/api/download/models/{model_version['id']}?type={urllib.parse.quote(file['type'])}&format={urllib.parse.quote(file['format'])}")
			# Add any other files to the output
			if j == 0:
				out.append(f"[**Download ({format_kb(file['sizeKB'])})**]({download_url})")
			elif file['type'] == 'Model' or file['type'] == 'Pruned Model':
				pruned = ""
				if file['type'] == 'Pruned Model':
					pruned = " Pruned"
				out.append(f" **|** [**{file['format']}{pruned}**]({download_url})")
			else:
				out.append(f" **|** [**{file['type']}**]({download_url})")

		out.append(f"\n\n</div>\n\n")
		# Add the model version name and other data to the output string
		out.append(f"Version|{model_version['name']}\n")
		out.append("-|-\n")
		if 'rank' in model_version:
			out.append(f"Rating|{generate_stars(model_version['rank']['ratingAllTime'], 5)} ({model_version['rank']['ratingCountAllTime']})\n")
			out.append(f"Downloads|{model_version['rank']['downloadCountAllTime']}\n")
		out.append(f"Uploaded|{format_date(model_version['createdAt'])}\n")
		if model_version['trainedWords']:
			# Concatenate all trained words into a string and wrap them with tilda (`) characters separating with commas
			trigger_words_string = f"`</span> <span style='{trigger_words_style}'>`".join(model_version['trainedWords'])
			out.append(f"Trigger Words| <span style='{trigger_words_style}'>`{trigger_words_string}`</span>\n")
		if model_version['baseModel'] and model_version['baseModel'] is not None:
			out.append(f"Base Model|{model_version['baseModel']}\n")
		out.append(f"Format|{model_version['files'][0]['format']}\n\n")
		if model_version['description']:
			# Convert the description to markdown and check again to see if the new string is empty before adding to the output
			description = html_to_markdown(model_version['description'])
			if description and description != "":
				out.append(f"<details open>\n<summary>About this version</summary>\n\n")
				out.append(f"{description}\n")
				out.append(f"\n</details>\n")
		out.append(f"\n")
		# Loop through the images for this version
		for j, model_version_image in enumerate(model_version['images']):
			# Add the current image to the output string
			out.append(f"{img_md(model_version_image.get('image', model_version_image))}\n\n")
		# Add a horizontal rule to separate the model versions from the rest of the document
		out.append(f"\n</details>\n\n***\n")
		first_version = False
	# Add a section to the markdown doc for reviews, if there are any
	masonry_style = "display: grid; align-items: flex-start; align-content: stretch; gap: 1em; max-width: 100%; overflow: hidden; justify-content: center; grid-template-columns: repeat(auto-fill, minmax(420px, 1fr)); grid-template-rows: masonry;"
	if reviews:
		out.append(f"## Discussion\n\n<div style='{masonry_style}'>\n\n")
		review_index = 0
		# Iterate through the reviews
		for review in reviews:
			review_index += 1
			# Only include the review/comment if it has an image or text
			if ("text" not in review or review["text"] is None) and ("content" not in review or review["content"] is None) and ("images" not in review or len(review["images"]) == 0):
				continue
			# Add the username as a subheading
			if review_index % 2 == 0:
				review_color = "#c5dbf2"
			else:
				review_color = "#dce7f2"
			out.append(f"<div style='vertical-align: top; display: inline-block; width: 440px; background: {review_color}; padding: 10px; margin: 10px; border-radius: 5px;'>\n\n")
			out.append(f"###### **{review['user']['username']}** ")
			if "rating" in review and review["rating"] is not None:
				out.append(f'<span style="float: right">{generate_stars(review["rating"], 5)}</span>')
			out.append(f"\n\n")
			# Add the review text, if it exists
			if "text" in review and review["text"] is not None:
				out.append(f"{html_to_markdown(review['text'])}\n")
			elif "content" in review and review["content"] is not None:
				out.append(f"{html_to_markdown(review['content'])}\n")
			# Add the images for the review if they exist (it might be just a comment)
			if "images" in review and review["images"] is not None:
				for review_image in review["images"]:
					if review_image.get("skip", False):
						continue
					out.append(f"{img_md(review_image)}\n")
			# Add a horizontal rule to separate the reviews
			out.append(f"</div>\n")
		out.append(f"</div>\n\n***\n")
	# Add a section for the source
	out.append(f"<div style='text-align: center'>\n\n[*source*](https://civitai.com/models/{data['id']})\n</div>\n")
	return "".join(out)
	
def generate_html_file(data, reviews):
	out = [f'<!DOCTYPE html>\n<html>\n<head>\n']
	out.append(f'<title>{data["name"]}</title>\n')
	out.append(f'<meta http-equiv="content-type" content="text/html; charset=UTF-8" />\n')
	out.append(f'<meta name="viewport" content="width=device-width, initial-scale=1" />\n')
	out.append('''<style>
	a {
		color: #1564df;
	}
	.page-container {
		position: relative;
		max-width: 1600px;
		margin: auto;
	}
	.page-heading {
		display: flex;
		flex-direction: row;
		flex-wrap: wrap;
		align-items: flex-start;
		gap: 10px;
		justify-content: space-between;
	}
	.page-heading h1 {
		flex: 1 0 100%;
	}
	.page-heading > div:last-of-type {
		flex: 0;
	}
	.likes-and-ranks {
		gap: 10px;
		display: flex;
		flex: 0;
	}
	.likes-and-ranks span {
		background: #eee;
		padding: 5px;
		white-space: nowrap;
	}
	h1 {
		text-align: center;
	}
	.type {
		background: rgb(231, 245, 255);
		color: rgb(34, 139, 230);
		padding: 5px;
		font-weight: bold;
		border-radius: 5px;
	}
	body {
		font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Helvetica, Arial, sans-serif, Apple Color Emoji, Segoe UI Emoji;
	}
	.img-meta {
		display: none;
	}
	.img-meta-ico::before {
		content: 'ⓘ';
		color: white;
		text-shadow: 1px 1px 1px black;
		padding: 0 6px;
		font-size: 22px;
		cursor: pointer;
		background: red;
		border-radius: 5px;
		display: grid;
		justify-content: center;
		align-content: center;
		overflow: inherit;
		position: absolute;
		top: 0;
		right: 0;
	}
	.img-meta-ico {
		display: inline-block;
		opacity: 0;
		transition: opacity 0.3s;
	}
	.img-container:hover .img-meta-ico {
		opacity: 1;
	}
	.img-set-container {
		display: grid;
		gap: 10px;
		grid-template-columns: min-content;
		grid-template-rows: min-content;
		align-content: center;
		align-items: stretch;
		overflow: auto;
		width: min-content;
		margin: auto;
		max-width: 100%;
	}
	.img-container {
		position: relative;
		border: 1px solid grey;
		width: 193px;
		height: 194px;
		overflow: hidden;
		background: #333;
	}
	.img-container:first-of-type,
	.version-img-set .img-container:nth-of-type(6),
	.version-img-set .img-container:nth-of-type(11),
	.version-img-set .img-container:nth-of-type(16)
	{
		width: 400px;
		height: 400px;
	}
	.comment .img-container:first-of-type,
	.comment .img-container:nth-of-type(4) {
		width: 400px;
		height: 400px;
	}
	.version-img-set .img-container:first-of-type {
		grid-area: 1 / 1 / 3 / 3;
	}
	.version-img-set .img-container:nth-of-type(6) {
		grid-area: 2 / 3 / 4 / 5;
	}
	.version-img-set .img-container:nth-of-type(11)
	{
		grid-area: 4 / 2 / 6 / 4;
	}
	.version-img-set .img-container:nth-of-type(16)
	{
		grid-area: 6 / 1 / 8 / 3;
	}
	.comment .img-container:first-of-type {
		grid-area: 1 / 1 / 1 / 3;
	}
	.comment .img-container:nth-of-type(4) {
		grid-area: 3 / 1 / 4 / 3
	}
	.img-container img {
		object-fit: contain;
		height: 100%;
		width: 100%;
		object-position: center;
		cursor: zoom-in;
	}
	.download {
		list-style: none;
		background: rgb(231, 245, 255);
		color: rgb(34, 139, 230);
		padding: 15px 20px;
		font-weight: bold;
		border-radius: 5px;
		flex: 1 100%;
	}
	.download li {
		display: inline-block;
	}
	.download li:not(:first-of-type)::before {
		content: "|";
		margin: 0 1em;
	}
	.download a {
		color: rgb(34, 139, 230);
	}
	table {
		border-spacing: 0;
		float: right;
		overflow: hidden;
		border: 1px solid grey;
		border-radius: 5px;
		margin-bottom: 1em;
		flex: 1;
	}
	th, td {
		text-align: left;
		padding: 10px;
		
	}
	tr:not(:first-of-type) > th, tr:not(:first-of-type) > td {
		border-top: 1px solid grey;
	}
	th {
		border-right: 1px solid grey;
		background-color: #d7d7d7;
		white-space: nowrap;
	}
	.tag {
		background: rgba(243, 240, 255, 1);
		color: #7950f2;
		border-radius: 5px;
		padding: 3px 8px;
		white-space: nowrap;
		cursor: pointer;
	}
	.tag-copy-ico {
		pointer-events: none;
	}
	.tag-container {
		display: flex;
		flex-wrap: wrap;
		gap: 3px 8px;
	}
	.version-container {
		display: flex;
		flex-wrap: wrap;
		gap: 10px;
		align-items: start;
		flex-direction: row-reverse;
	}
	.version-container > hr {
		flex: 1 100%;
	}
	.version-description {
		flex: 1 0 33%;
		display: flex;
		flex-direction: column;
	}
	h4 {
		font-size: 130%;
	}
	.comment:only-of-type {
		grid-area: 1 / 1 / 1 / 4;
	}
	.comment {
		margin: auto;
		box-shadow: 2px 2px 10px #ccc;
		border: 1px solid #ccc;
		padding: 0 15px 15px;
		border-radius: 5px;
		background: #dce7f2;
		width: 850px;
		max-width: 100%;
		position: relative;
		box-sizing:border-box;
	}
	.comment:nth-last-of-type(2n) {
		background: #c5dbf2
	}
	.comment .comment-heading > h5 {
		color: #1b94dd;
		margin: 0;
	}
	.comment-heading {
		display: flex;
		justify-content: space-between;
		align-items: center;
		flex-wrap: wrap;
		gap: 5px;
		margin: 1em 0;
	}
	h5 {
		font-size: 110%;
	}
	.comment-text {
		padding-bottom: 1em;
	}
	.reviews {
		display: grid;
		flex-wrap: wrap;
		align-items: flex-start;
		justify-content: stretch;
		align-content: stretch;
		flex-direction: column;
		gap: 1em;
		max-width: 100%;
		overflow: hidden;
		grid-template-columns: repeat(auto-fill, 505px);
		justify-content: center;
		align-items: flex-start;
		grid-template-columns: repeat(auto-fill, minmax(420px, 1fr));
		grid-template-rows: masonry;
	}
	.comment .img-set-container {
		overflow-y: auto;
		align-items: stretch;
		flex: 1 0 380px;
		box-sizing: border-box;
		margin-top: 15px;
		position: relative;
		scroll-snap-type: y mandatory;
	}
	.comment .img-container {
		flex: 1;
		box-sizing: border-box;
		max-width: 100%;
		max-height: 100%;
		display: flex;
		position: relative;
		scroll-snap-align: start;
	}
	.comment .img-container img {
		flex: 1 1 min-content;
		background: black;
		object-fit: contain;
		object-position: center;
	}
	.source {
		padding: 10px;
		text-align: center;
	}
	.source a {
		color: #55f;
		text-decoration: none;
		font-style: italic;
	}
	.version-img-set {
		flex: 1 0 min-content;
		max-width: 100%;
	}
	hr {
		opacity: 0.2;
	}
	/* tab related css */
	.version-container:not(.active) {
		display: none;
	}
	/* Style the tab */
	.version-tabs {
		overflow: hidden;
	}
	/* Style the heading of the tabs */
	.version-tabs h1 {
		font-size: 100%;
		background-color: inherit;
		float: left;
		border: none;
		outline: none;
		padding: 0 16px;
		user-select: none;
	}
	/* Style the buttons that are used to open the tab content */
	.version-tabs button {
		background-color: inherit;
		float: left;
		border: none;
		outline: none;
		cursor: pointer;
		padding: 14px 16px;
		transition: 0.3s;
		border-bottom: 2px solid #ccc;
		border-radius: 4px 4px 0px 0px;
	}
	/* Change background color of buttons on hover */
	.version-tabs button:hover {
		background-color: rgb(248, 249, 250);
	}
	/* Create an active/current tablink class */
	.version-tabs button.active {
		border-color: rgb(34, 139, 230);
		background-color: rgb(248, 249, 250);
	}
	/* Zoom Styling */
	#overlay {
		position: fixed;
		top: 0;
		left: 0;
		width: 100%;
		height: 100%;
		background: rgba(0, 0, 0, 0.9);
		z-index: 999;
		visibility: hidden;
		opacity: 0;
		transition: all 0.2s ease-in-out;
		cursor: zoom-out;
	}
	#overlay.visible {
		visibility: visible;
		opacity: 1;
	}
	#overlay img {
		width: 90%;
		height: 90%;
		position: absolute;
		top: 50%;
		left: 50%;
		transform: translate(-50%, -50%);
		object-fit: contain;
	}
	</style>
	''')
	model_versions = sorted(data['modelVersions'], key=lambda x: x['createdAt'], reverse=True)
	out.append(f'</head>\n<body>\n<div class="page-container"><div class="page-heading">')
	out.append(f'<div class="likes-and-ranks"><span>🤍 {data["rank"]["favoriteCountAllTime"]}</span><span>{generate_stars(data["rank"]["ratingAllTime"], 5)} {data["rank"]["ratingCountAllTime"]}</span></div>\n')
	out.append(f'<div class="type">{data["type"]}</div>\n')
	out.append(f'<h1>{data["name"]}</h1></div>\n')
	out.append(f'<div class="img-set-container">')
	out.append(img_html(get_best_main_image(model_versions)))
	out.append(f'</div>\n')
	out.append(f'<div class="description">{data["description"]}</div>\n\n')
	out.append('<div class="version-tabs">\n<h1>Versions:</h1>')
	versions_out = []
	for i, model_version in enumerate(model_versions):
		out.append(f'<button version-id="model-version-{model_version["id"]}"' + ( ' class="active"' if i == 0 else "" ) + f'>{model_version["name"]}</button>\n')
		versions_out.append(f'<div id="model-version-{model_version["id"]}" class="version-container{ " active" if i == 0 else "" }">\n')
		versions_out.append(f'<ul class="download">\n')
		for j, file in enumerate(model_version["files"]):
			hash_av1 = get_file_hash(file)
			hash_av2 = get_file_hash(file, "AutoV2")
			hash_sha256 = get_file_hash(file, "SHA256")
			if hash_av2 is None and hash_sha256 is not None:
				hash_av2 = hash_sha256[:10]
			hash = hash_av2 if hash_av2 is not None else hash_av1
			hash_tag = []
			if hash is not None:
				hash_tag.append(f'title="Hash: {hash}"')
			if hash_av1 is not None:
				hash_tag.append(f'autoV1="{hash_av1}"')
			if hash_av2 is not None:
				hash_tag.append(f'autoV2="{hash_av2}"')
			if hash_sha256 is not None:
				hash_tag.append(f'sha256="{hash_sha256}"')
			if j == 0:
				tag = f'Download ({format_kb(model_version["files"][0]["sizeKB"])})'
			elif model_version['files'][j]['type'] == 'Model' or model_version['files'][j]['type'] == 'Pruned Model':
				pruned = ""
				if model_version['files'][j]['type'] == 'Pruned Model':
					pruned = " Pruned"
				tag = file["format"] + pruned
			else:
				tag = file["type"]
			download_url = file.get('downloadUrl', f'https://civitai.com/api/download/models/{model_version["id"]}?type={file["type"]}&format={file["format"]}')
			versions_out.append(f'<li><a href="{download_url}" {" ".join(hash_tag)} target="_blank" rel="noopener noreferrer">{tag}</a></li>\n')
		versions_out.append(f'</ul>\n')
		
		versions_out.append(f'<div class="version-description"><table>\n')
		if 'rank' in model_version:
			versions_out.append(f'<tr><th>Rating</th><td>{generate_stars(model_version["rank"]["ratingAllTime"], 5)} ({model_version["rank"]["ratingCountAllTime"]})</td></tr>\n')
			versions_out.append(f'<tr><th>Downloads</th><td>{model_version["rank"]["downloadCountAllTime"]}</td></tr>\n')
		versions_out.append(f'<tr><th>Uploaded</th><td>{format_date(model_version["createdAt"])}</td></tr>\n')
		if model_version["trainedWords"]:
			versions_out.append('<tr><th>Trigger Words</th><td><div class="tag-container">')
			for word in model_version["trainedWords"]:
				versions_out.append('<span class="tag">')
				versions_out.append(word)
				versions_out.append(' <span class="tag-copy-ico"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><rect x="8" y="8" width="12" height="12" rx="2"></rect><path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path></svg></span>')
				versions_out.append('</span>')
			versions_out.append('</div></td></tr>\n')
		if model_version["baseModel"] and model_version["baseModel"] is not None:
			versions_out.append(f'<tr><th>Base Model</th><td>{model_version["baseModel"]}</td></tr>\n')
		versions_out.append(f'<tr><th>Format</th><td>{model_version["files"][0]["format"]}</td></tr>\n')
		versions_out.append(f'</table>\n')
		if model_version["description"] and model_version["description"] != "":
			versions_out.append(f'<div class="version-description-text">{model_version["description"]}</div>\n')
		versions_out.append('</div>')

		versions_out.append(f'<div class="version-img-set"><div class="img-set-container">')
		for j, image in enumerate(model_version["images"]):
			versions_out.append(img_html(image.get("image", image)))
		versions_out.append(f'</div></div>\n')

		versions_out.append(f'\n</div>')
	out.append('</div>\n')
	out.append(''.join(versions_out))

	# Add a section to the markdown doc for reviews, if there are any
	if reviews:
		out.append(f'<h4>Discussion</h4>\n<div class="reviews">')
		# Iterate through the reviews
		for review in reviews:
			# Only include the review/comment if it has an image or text
			if ("text" not in review or review["text"] is None) and ("content" not in review or review["content"] is None) and ("images" not in review or len(review["images"]) == 0):
				continue
			# Get the create time
			date_string = review["createdAt"]
			date = datetime.strptime(date_string, "%Y-%m-%dT%H:%M:%S.%fZ")
			# Convert the datetime object to a timestamp (integer)
			timestamp = date.timestamp()
			# Add the username as a subheading
			out.append(f'<div class="comment" style="order: {int(timestamp)*-1}"><div class="comment-heading"><h5 title-time-stamp="{date_string}">{review["user"]["username"]}</h5>\n')
			if "rating" in review and review["rating"] is not None:
				out.append(f'<div class="comment-rating">{generate_stars(review["rating"], 5)}</div>')
			out.append(f'</div>\n')
			# Add the review text, if it exists
			if "text" in review and review["text"] is not None:
				out.append(f'<div class="comment-text">{review["text"]}</div>\n')
			elif "content" in review and review["content"] is not None:
				out.append(f'<div class="comment-text">{review["content"]}</div>\n')
			# Add the images for the review if they exist (it might be just a comment)
			if "images" in review and review["images"] is not None and len(review["images"]) > 0:
				out.append(f'<div class="img-set-container">')
				for review_image in review["images"]:
					if review_image.get("skip", False):
						continue
					out.append(img_html(review_image))
				out.append(f'</div>')
			out.append("</div>\n")
		out.append("</div>\n")
	# Include these links just to have handy, they will not appear in the markdown preview because their hypertext is empty
	out.append("<!-- https://rentry.org/ https://ghostarchive.org/ -->")
	# Add a section for the source and archive links, hide the archive link by default. Remove the wrapping [​]() to make visible
	out.append(f'<div class="source"><a href="https://civitai.com/models/{data["id"]}" target="_blank" rel="noopener noreferrer">Source</a> | <a href="https://civitai.com/user/{data.get("creator", data.get("user", {}))["username"]}" target="_blank" rel="noopener noreferrer">Author</a></div>\n')
	out.append("</div>\n")
	out.append("""<script>
	function utcToLocalDatetimeString(utcTimestamp) {
		if(typeof utcTimestamp == 'undefined') return '';
		return new Date(utcTimestamp).toLocaleString();
	}

	setTitles = document.querySelectorAll('h5[title-time-stamp]');
	setTitles.forEach(title => {
		title.setAttribute('title', utcToLocalDatetimeString(title.getAttribute('title-time-stamp')));
	});

	const images = document.querySelectorAll(".img-container img");
	const overlay = document.createElement("div");
	overlay.setAttribute("id", "overlay");
	document.body.appendChild(overlay);

	images.forEach(image => {
		image.addEventListener("click", function() {
			overlay.innerHTML = `<img src="${image.src}">`;
			overlay.classList.add("visible");
		});
	});

	overlay.addEventListener("click", function() {
		overlay.classList.remove("visible");
	});

	document.querySelectorAll('.version-tabs button').forEach(button => {
		button.addEventListener('click', event => {
			document.querySelectorAll('.version-container').forEach(version => {
				version.classList.remove("active");
			});			
			document.querySelectorAll('.version-tabs button').forEach(btn => {
				btn.classList.remove("active");
			});
			event.target.classList.add("active");
			const versionAttr = event.target.getAttribute("version-id");

			let version = versionAttr.split("-").pop();
			history.replaceState(null, null, "#" + version);

			const versionDiv = document.getElementById(versionAttr);
			versionDiv.classList.add("active");
		});
	});

	document.querySelectorAll('.tag-container .tag').forEach(tag => {
		tag.addEventListener('click', event => {
			navigator.clipboard.writeText(event.target.textContent.trim());
		});
	});

	document.querySelectorAll('.img-meta-ico').forEach(ico => {
		ico.addEventListener('click', event => {
			const textarea = event.target.nextElementSibling;
			navigator.clipboard.writeText(textarea.value);
		});
	});

	document.addEventListener("DOMContentLoaded", function(event) { 
		if( window.location.hash ) {
			const hash = window.location.hash.substring(1);
			const activeTab = document.querySelector(`button[version-id="model-version-${hash}"]`)
			if (activeTab){
				activeTab.click()
			}
		}
	});
	</script>
	""")
	# Convert the dictionary to a JSON string
	json_data = json.dumps(rebuildSourceJSON(data, reviews))
	out.append(f'<script type="application/json">{json_data}</script>')
	out.append("</body>\n</html>")
	return "".join(out)

def getPageDataFromModelData(modelData):
	# Get the main page json data
	pageData = modelData["pageData"]

	# get the relevant data from json
	return pageData["props"]["pageProps"]["trpcState"]["json"]["queries"][0]["state"]["data"]

def getReviewsFromModelData(modelData):
	# Get the main reviews json data
	reviewsJson = modelData["reviewData"]

	# hold the relevant review data
	reviews = []

	# Loop through the JSON array
	for reviewsJson_item in reviewsJson:
		# Get the reviews from the current object (Doing it this way incase more than one array object has reviews in it)
		reviewsTemp = reviewsJson_item.get('result', {}).get('data', {}).get('json', {}).get('reviews', None)
		if reviewsTemp:
			# Concatenate the reviews onto the main reviews array
			reviews += reviewsTemp
		else:
			# Check for comments without images (Doing it this way incase more than one array object has comments in it)
			commentsTemp = reviewsJson_item.get('result', {}).get("data", {}).get("json", {}).get("comments", [])
			if commentsTemp:
				# Concatenate the comments onto the main reviews array
				reviews += commentsTemp
	
	return reviews

def rebuildReviewsJson(reviews):
	reviewsJson = []
	comments = []
	for review in reviews:
		if "rating" in review:
			# This is a review
			reviewsJson.append(review)
		else:
			# This is a comment
			comments.append(review)
	return [
		{ "result": { "data": { "json": { "reviews": reviewsJson } } } },
		{ "result": { "data": { "json": { "comments": comments } } } }
	]

def rebuildSourceJSON(data, reviews):
	out = {}
	page_data = {
		"props": {
			"pageProps": {
				"trpcState": {
					"json": {
						"queries": [
							{
								"state": {
									"data": data
								}
							}
						]
					}
				}
			}
		}
	}
	out[str(data["id"])] = {
		"pageData": page_data,
		"reviewData": rebuildReviewsJson(reviews)
	}
	return out

def get_filename_for_url(url):
	if not url:
		return None

	headers = {'Range': 'bytes=0-0'}
	response = requests.get(url, headers=headers, allow_redirects=True)

	cd = response.headers.get('content-disposition')

	if not cd:
		return None

	fname = re.findall('filename=(.+)', cd)

	if len(fname) == 0:
		return None

	fname = fname[0].strip('"')

	if len(fname) == 0:
		return None

	return fname

def generate_version_files_array(data):
	extensions = [".ckpt",".safetensors",".pt",".bin"]
	sanitized_name = sanitize_filename(data['name'], file_name_max_size)
	html = f"{sanitized_name}_({data['id']}).html"
	folder_name = get_folder(data['type'])
	for i, model_version in enumerate(data["modelVersions"]):
		update_version_files = False
		files = set()
		hashes = []
		version_names = set()
		id = str(model_version['id'])
		sanitized_version_name = sanitize_filename(model_version['name'], file_name_max_size)
		image = None
		if model_version["images"]:
			image = get_image_key(model_version["images"][0].get("image", model_version["images"][0]))
		for file in model_version["files"]:
			file_name = file["name"]
			file_type = file['type']
			file_format = file['format']

			# get the file names for the files associated with the models
			if any(file_name.lower().endswith(ext) for ext in extensions) and file_type.lower() != "vae":
				update_version_files = True
				temp_hashes = []

				# if the file has a dictionary of hashes convert them to an array of objects in {type:x, hash:y} format
				file_hashes = file.get("hashes", [])
				if type(file_hashes) == dict:
					for key, value in file_hashes.items():
						temp_hashes.append({ "type": key, "hash": value })
				else:
					temp_hashes.extend(file_hashes)

				hashes.extend(temp_hashes)
				
				if sanitized_version_name not in version_names:
					version_names.add(sanitized_version_name)

				file_name_em = file_name[:file_name.rindex(".")] + "_em" + file_name[file_name.rindex("."):]
				
				if file_name not in files:
					files.add(file_name)

				if file_name_em not in files and data["type"].lower() == "textualinversion":
					files.add(file_name_em)

				# the old system needed you to download the file to get the models's proper file name. The new system has the proper file name in file['name']. Use the existence of the new property 'downloadUrl' to check if its the old version or not
				if 'downloadUrl' not in file:
					
					if id in version_files.keys() and sanitized_name == version_files[id]['name'] and sanitized_version_name in version_files[id]['versions']:
						continue # odds that the filename has changed since the last time it was saved is low, skip this file

					download_url = file.get('downloadUrl', f"https://civitai.com/api/download/models/{model_version['id']}?type={file_type}&format={file_format}")
					url_file_name = get_filename_for_url(download_url)

					if url_file_name is not None and file_name != url_file_name and url_file_name not in files:
						url_file_name_em = url_file_name[:url_file_name.rindex(".")] + "_em" + url_file_name[url_file_name.rindex("."):]
						files.add(url_file_name)
						if data["type"] == "TextualInversion" and url_file_name_em not in files:
							files.add(url_file_name_em)

		if update_version_files:
			hash_set = set()
			new_hashes = []
			for hash in hashes:
				hash_tup = (hash["type"].lower(), hash["hash"].lower())
				if hash_tup not in hash_set:
					hash_set.add(hash_tup)
					new_hashes.append(hash)
				
			new_hashes = sorted(new_hashes, key=lambda h: (h['type'].lower(), h['hash'].lower()))

			update = {}
			update[id] = {
				'name': sanitized_name,
				'versions': list(version_names),
				'html': html,
				'hashes': new_hashes,
				'files': list(files),
				'folder': folder_name,
				'image' : image
			}

			if id in version_files.keys():
				version_files[id]['name'] = update[id]['name']
				version_files[id]['versions'] = sorted(list(set(version_files[id]['versions']).union(version_names)))
				version_files[id]['files'] = sorted(list(set(version_files[id]['files']).union(files)))
				version_files[id]['html'] = update[id]['html']
				version_files[id]['folder'] = update[id]['folder']
				version_files[id]['image'] = update[id]['image']

				if "hashes" in version_files[id]:
					for hash in version_files[id]['hashes']:
						if isinstance(hash, str):
							hash = { "type": "AutoV1", "hash": hash }
						hash_tup = (hash["type"].lower(), hash["hash"].lower())
						if hash_tup not in hash_set:
							hash_set.add(hash_tup)
							new_hashes.append(hash)
				

				new_hashes = sorted(new_hashes, key=lambda h: (h['type'].lower(), h['hash'].lower()))
				version_files[id]['hashes'] = new_hashes
			else:
				version_files.update(update)

def processJSONData(json_data, key_to_generate):
	# Create empty lists for the md and html files
	json_merge = {}

	keyCount = 0
	if args.debug:
		data_length = len(json_data)
	else:
		data_length = len(key_to_generate)
	# Loop over the keys in the json object
	for key in json_data:
		# Get the main page json data
		model_data = json_data[key]
		data = getPageDataFromModelData(model_data)
		reviews = getReviewsFromModelData(model_data)

		process = key in key_to_generate

		if args.debug or (args.print and process):
			keyCount += 1
		
		print_working_line = f"{data['name']} ({key}) ({keyCount}/{data_length})"

		if args.print and process:
			sys.stdout.write(print_working_line)
			sys.stdout.flush()
		elif args.debug:
			print(print_working_line)

		# Rebuild the json source files
		if args.debug:
			print(f"Rebuild the json source files {'only' if not process else ''}")
		rebuilt_json = rebuildSourceJSON(data, reviews)
		json_merge.update(rebuilt_json)
		json_string = json.dumps(rebuilt_json, indent=2)

		if process:
			# Clear the runtime image cache so that it doesn't get too big
			image_cache_set.clear()

			# Generate the md and html strings
			if args.md:
				if args.debug:
					print(f"Generate Markdown")
				if args.md_legacy:
					md_string = generate_md_file_legacy(data, reviews)
				else:
					md_string = generate_md_file(data, reviews)
			if args.html:
				if args.debug:
					print(f"Generate HTML")
				html_string = generate_html_file(data, reviews)

			if args.file_names:
				if args.debug:
					print(f"Generate/update the version files cache")
				# Generate/update the version files cache
				generate_version_files_array(data)
				
			if args.print and not args.debug:
				print(f" Creating files")

			folder_name = get_folder(data['type'])

			# Check if the directory exists
			if not os.path.exists(folder_name):
				# If it doesn't exist, create the directory
				os.makedirs(folder_name)
			
			md_path = os.path.join(folder_name, f"{sanitize_filename(data['name'], file_name_max_size)}_({key}).md")
			html_path = os.path.join(folder_name, f"{sanitize_filename(data['name'], file_name_max_size)}_({key}).html")
			json_path = os.path.join(folder_name, f"{sanitize_filename(data['name'], file_name_max_size)}_({key}).json")

			if args.debug:
				print(f"Writing files")

			if args.md:
				with open(md_path, 'w', encoding='utf-8') as f:
					f.write(md_string.encode('utf-8').decode('utf-8'))
				f.close()

			if args.html:
				with open(html_path, 'w', encoding='utf-8') as f:
					f.write(html_string.encode('utf-8').decode('utf-8'))
				f.close()

			if args.json:
				with open(json_path, 'w', encoding='utf-8') as f:
					f.write(json_string.encode('utf-8').decode('utf-8'))
				f.close()

	with open(args.combine, 'w', encoding='utf-8') as f:
		json_string = json.dumps(json_merge, indent=2)
		f.write(json_string.encode('utf-8').decode('utf-8'))
	f.close()

	if args.file_names:
		# Save the version files cache to a file
		with open('version_files.json', 'w') as f:
			json.dump(version_files, f, indent=2)
		f.close()

def update_data(old_data, new_data):
	# Iterate over the keys in the new data
	for key in new_data:
		# See if there was an error and skip if there was an error
		new_state = new_data[key].get('pageData', {}).get('props', {}).get('pageProps', {}).get('trpcState', {}).get('json', {}).get('queries')[0].get('state', {})
		if new_state.get('status', "").lower() == "error":
			print(f"Error while building {key} - {new_state.get('error', {}).get('message', 'UNKNOWN ERROR')}")
			continue

		# If the key exists in the old data, update the old data with the new data
		if key in old_data:
			# Update the pageData
			try:
				old_model_versions = old_data[key]['pageData']['props']['pageProps']['trpcState']['json']['queries'][0]['state']['data']['modelVersions']
				new_model_versions = new_data[key]['pageData']['props']['pageProps']['trpcState']['json']['queries'][0]['state']['data']['modelVersions']
			except Exception as e:
				print(f"ERROR with {key} - {e}")
				exit()

			# check if new model version already exists in old data, if not, extend the array
			for new_version in new_model_versions:
				version_found = False
				for old_version in old_model_versions:
					if new_version['id'] == old_version['id']:
						version_found = True
						# update rank if available
						new_rank = new_version.get('rank', None)

						if new_rank:
							old_version['rank'] = new_rank

						old_images = old_version.get('images', [])
						new_images = new_version.get('images', [])
						
						# extend images array and add any new images
						for new_image in new_images:
							image_found = False
							for old_image in old_images:
								if is_same_image_id(new_image, old_image):
									image_found = True
									break
							if not image_found:
								old_images.append(new_image)
						old_version['images'] = old_images
						break
						
				if not version_found:
					old_model_versions.append(new_version)

			# Update the pageData
			old_data[key]['pageData'] = new_data[key]['pageData']

			# Put back the old model versions
			old_data[key]['pageData']['props']['pageProps']['trpcState']['json']['queries'][0]['state']['data']['modelVersions'] = old_model_versions
			
			# Update the comments
			old_data[key]['reviewData'][1] = new_data[key]['reviewData'][1]
			# Iterate over the list of reviews in the new data
			for new_review in new_data[key]['reviewData'][0]['result']['data']['json']['reviews']:
				# Check if the review already exists in the old data
				review_exists = False
				for old_review in old_data[key]['reviewData'][0]['result']['data']['json']['reviews']:
					if old_review['id'] == new_review['id']:
						review_exists = True
						break
				# If the review doesn't exist, append it to the old data's list of reviews
				if not review_exists:
					old_data[key]['reviewData'][0]['result']['data']['json']['reviews'].append(new_review)
				# If the review does exist, merge the images lists
				else:
					# Iterate over the list of images in the new review
					for new_image in new_review['images']:
						# Check if the image already exists in the old review's list of images
						image_exists = False
						for old_image in old_review['images']:
							if is_same_image_id(new_image, old_image):
								image_exists = True
								break
						# If the image doesn't exist, append it to the old review's list of images
						if not image_exists:
							old_review['images'].append(new_image)
		# If the key doesn't exist in the old data, add it
		else:
			old_data[key] = new_data[key]

def buildCommentParams(key, model_num, limit, source = {}, cursor = None):
	if limit > 100:
		limit = 100
	if cursor is not None:
		cursor = int(cursor)
	out = {
		key: {
			"json": {
				"modelId": int(model_num),
				"limit": int(limit),
				"sort": "newest",
				"cursor": cursor
			}
		}
	}
	
	if cursor is None:
		x = {
			"meta": {
				"values": {
					"cursor": ["undefined"]
				}
			}
		}
		out[key].update(x)
		
	source.update(out)
	return source


def getNewDataFromSiteForModel(model_num):
	# Make sure it's a valid model number
	if model_num and int(model_num) > 0:
		model_num = str(model_num)
		# Set the value of the url_model variable to the URL of the model page
		url_model = f"https://civitai.com/api/v1/models/{model_num}/"
		# build the comment params url string
		comment_params = urllib.parse.quote(json.dumps(buildCommentParams(1, model_num, comment_limit, buildCommentParams(0, model_num, review_limit))))
		# Set the value of the url_reviews variable to the URL of the review data
		url_reviews = f"https://civitai.com/api/trpc/review.getAll,comment.getAll?batch=1&input={comment_params}"
		# Send a GET request to the model page and parse the response
		response = requests.get(url_model, timeout=(10, 20))

		if response.status_code == 200:
			doc = response.json()
			if "error" in doc:
				print(doc['error'])
				exit() # may want to change this to a soft fail
		else:
			print("Error: Invalid response from server")
			print(response)
			exit() # may want to change this to a soft fail

		# Insert page data back into old structure for backwards compatibility
		page_data = { 'props': { 'pageProps': { 'trpcState': { 'json': { 'queries': [ { 'state': { 'data': doc } } ] } } } } }

		headers = {
			"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/110.0",
			"Accept": "*/*",
			"Accept-Language": "en-CA,en-US;q=0.7,en;q=0.3",
			"Accept-Encoding": "utf-8",
			"Referer": f"https://civitai.com/models/{model_num}",
			"content-type": "application/json",
			"DNT": "1",
			"Connection": "keep-alive",
			#"Cookie": "COOKIE HERE",
			"Sec-Fetch-Dest": "empty",
			"Sec-Fetch-Mode": "cors",
			"Sec-Fetch-Site": "same-origin",
			"TE": "trailers",
		}

		if cookie:
			headers['Cookie'] = cookie

		# Send a GET request to the review page and parse the response
		response = requests.get(url_reviews, headers=headers, timeout=(10, 20))
		review_data = response.json()

		review_count = 0
		nextCursor = None
		reviewKey = 0

		reviews = review_data[reviewKey].get('result', {}).get('data', {}).get('json', {}).get('reviews', None)
		if reviews is None:
			reviewKey = 1
			reviews = review_data[reviewKey].get('result', {}).get('data', {}).get('json', {}).get('reviews', None)

		if reviews is not None:
			review_count = len(reviews)
			nextCursor = review_data[reviewKey].get('result', {}).get('data', {}).get('json', {}).get('nextCursor', None)

		while nextCursor is not None and review_count < review_limit:
			if args.print:
				print(f"Getting next bunch of reviews. Count at: {review_count}. Limit: {review_limit}")
			comment_params = urllib.parse.quote(json.dumps(buildCommentParams(0, model_num, review_limit, cursor=nextCursor)))
			# Set the value of the url_reviews variable to the URL of the review data
			url_reviews = f"https://civitai.com/api/trpc/review.getAll?batch=1&input={comment_params}"
			response = requests.get(url_reviews, headers=headers, timeout=(10, 20))
			new_review_data = response.json()
			new_reviews = new_review_data[0].get('result', {}).get('data', {}).get('json', {}).get('reviews', [])
			nextCursor = new_review_data[0].get('result', {}).get('data', {}).get('json', {}).get('nextCursor', None)
			review_count += len(new_reviews)
			print(f"Cursor = {nextCursor}")
			for review in new_reviews:
				main_reviews = review_data[0].get('result', {}).get('data', {}).get('json', {}).get('reviews', [])
				if review["id"] not in [r["id"] for r in main_reviews]:
					main_reviews.append(review)


		# Add the data to the data object
		new_data = {}
		new_data[model_num] = {
			"pageData": page_data,
			"reviewData": review_data
		}

		# Sanitize the reviews/comments object by processing them and then un-processing them
		new_data[model_num]["reviewData"] = rebuildReviewsJson(getReviewsFromModelData(new_data[model_num]))

		return new_data


#################################################################################################################
################################################## SCRIPT #######################################################
#################################################################################################################

# Initialize the data object
data = {}

countArgs = 0
keys_to_generate = set()

# If the value is a file path, load the JSON data from the file and add it to the data object
if os.path.isfile(args.combine):
	if args.debug:
		print(f"Loading Combine {args.combine}")
	with open(args.combine, 'r') as f:
		file_data = json.load(f)
	f.close()
	
	countKey = 0
	fileDataLen = len(file_data)
	# Sanitize the reviews/comments object by processing them and then un-processing them
	for key in file_data:
		countKey += 1
		if args.debug:
			print(f"Sanitize reviews/comments")
		file_data[key]["reviewData"] = rebuildReviewsJson(getReviewsFromModelData(file_data[key]))
		
	if args.debug:
		print(f"Update json")
	# Update the data dictionary with the new data
	update_data(data, file_data)

for arg in args.args:
	countArgs += 1
	# If the value is a file path, load the JSON data from the file and add it to the data object
	if os.path.isfile(arg):
		if args.debug:
			print(f"Loading {arg}")
		with open(arg, 'r') as f:
			file_data = json.load(f)
		f.close()
		
		countKey = 0
		fileDataLen = len(file_data)
		# Sanitize the reviews/comments object by processing them and then un-processing them
		for key in file_data:
			countKey += 1
			keys_to_generate.add(key)
			if args.debug:
				print(f"Sanitize reviews/comments")
			file_data[key]["reviewData"] = rebuildReviewsJson(getReviewsFromModelData(file_data[key]))
			
			if args.update:
				if args.print or args.debug:
					print(f"Downloading {key} ({countArgs}.{countKey}/{len(args.args)}.{fileDataLen})")
				# Request the new data from the website for this model
				new_data = getNewDataFromSiteForModel(key)
				if args.debug:
					print(f"Update model")
				# Update the data dictionary with the new data
				update_data(file_data, new_data)
		
		if args.debug:
			print(f"Update json")
		# Update the data dictionary with the new data
		update_data(data, file_data)

	# Otherwise, assume it's a model number and send a GET request for the data
	else:
		keys_to_generate.add(arg)
		if not args.only_update_missing or (args.only_update_missing and arg not in data):
			if args.print or args.debug:
				print(f"Downloading {arg} ({countArgs}/{len(args.args)})")
			# Request the new data from the website for this model
			new_data = getNewDataFromSiteForModel(arg)
			# Update the data dictionary with the new data
			update_data(data, new_data)

if args.debug:
	print(f"Process json")
# Run the code to process the data
processJSONData(data, keys_to_generate)

# output the elapsed time
elapsed_time = time.time() - start_time
print(f"[INFO] Script completed in {elapsed_time:.2f} seconds")